\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,graphicx}
\usepackage{cleveref}


% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}

\definecolor{output_background}{rgb}{0.4, 0.4, 0.4}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}
\definecolor{purple}{rgb}{0.6, 0.1, 0.9}
\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=python,
breaklines=true,
basicstyle=\ttfamily\small,
otherkeywords={1, 2, 3, 4, 5, 6, 7, 8 ,9 , 0, -, =, +, [, ], (, ), \{, \}, :, *, !},             % Add keywords here
keywordstyle=\color{blue},
emph={class, pass, in, for, while, if, is, elif, else, not, and, or, OR
    def, print, exec, break, continue, return},
emphstyle=\color{black}\bfseries,
emph={[2]True, False, None, self},
emphstyle=[2]\color{purple},
emph={[3]from, import, as},
emphstyle=[3]\color{blue},
stringstyle=\color{red},
frame=tb,
showstringspaces=false,
morecomment=[s]{"""}{"""},
commentstyle=\color{gray},
rulesepcolor=\color{blue},
title=\lstname
}}

% Python style for output highlighting
\newcommand\pythonoutputstyle{\lstset{
backgroundcolor=\color{output_background},
basicstyle=\ttm\small\color{white},
showstringspaces=false
}}


\lstnewenvironment{pythonOutput}[1][]
{
\pythonoutputstyle
\lstset{#1}
}
{}

% Python for inline
\newcommand\pythonoutput[1]{{\pythonoutputstyle\lstinline!#1!}}


% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

\title{Assignment 1\\02807 Computational Tools for Big Data}
\author{Anonymous Authors}
\date{28th September 2015}
\usepackage[cm]{fullpage}
\begin{document}

\maketitle
\newpage
\section{Exercise 1.1}
\textbf{Write a command that finds the 10 most popular words in a file.}
\lstinputlisting[language=bash,breaklines=true, basicstyle=\ttfamily\small, frame=tb, title=\lstname]{"../Lesson 1/exercise1-1.sh"}

A file containing five paragraphs of lorem ipsum\footnote{http://pastebin.com/SMZMZzA7} will thus return:
\begin{pythonOutput}
     10 id
      9 in
      9 a
      8 eu
      7 ligula
      7 ipsum
      7 et
      7 ac
      6 vehicula
      6 tincidunt
\end{pythonOutput}
\section{Exercise 1.2}
\textbf{Put this data (https://www.dropbox.com/s/d5c4x905w4jelbu/cars.txt?dl=0) into a file and write a command that removes all rows where the price is more than 10,000\$.}
\lstinputlisting[language=bash,breaklines=true, basicstyle=\ttfamily\small, frame=tb, title=\lstname]{"../Lesson 1/exercise1-2.sh"}
Running it on the cars.txt file produces the following:
\begin{pythonOutput}
plym    fury    77      73      2500
chevy   nova    79      60      3000
volvo   gl      78      102     9850
Chevy   nova    80      50      3500
fiat    600     65      115     450
honda   accord  81      30      6000
toyota  tercel  82      180     750
chevy   impala  65      85      1550
ford    bronco  83      25      9525
\end{pythonOutput}


\section{Exercise 1.3}
\textbf{Using this file (https://www.dropbox.com/s/85fbd4l8s52f7to/dict?dl=0) as a dictionary, write a simple spellchecker that takes input from stdin or a file and outputs a list of words not in the dictionary. One solution gets 721 misspelled words in this Shakespeare file (https://www.dropbox.com/s/bnku7grfycm8ii6/shakespeare.txt?dl=0).}
\lstinputlisting[language=bash,breaklines=true, basicstyle=\ttfamily\small, frame=tb, title=\lstname]{"../Lesson 1/exercise1-3.sh"}
Executing the script and pipe it to head, gives:
\begin{pythonOutput}
abused
abuses
accoutrements
acres
actions
acts
adam
affairs
affections
ages
\end{pythonOutput}
\section{Exercise 1.4}
\textbf{Launch a t2.micro instance on Amazon EC2. Log onto the instance, create some files and install some software (for example git).}
\section{Exercise 1.5}
\textbf{Create a few files locally on your computer. Create a new repository on Github and push your files to this repository. Log on to a t2.micro instance on Amazon EC2 and clone your repository there. Make some changes to the files, push them again and pull the changes on your local machine.}

\section{Exercise 2.1}
\textbf{Write a script with two methods. The first method should read in a matrix like the one here and return a list of lists. The second method should do the inverse, namely take, as input, a list of lists and save it in a file with same format as the initial file. The first method should take the file name as a parameter. The second method should take two arguments, the list of lists, and a filename of where to save the output.}
\pythonexternal{"../Lesson 2/exercise2-1.py"}
We can first load an array from a file with the following content:
\begin{pythonOutput}
0 1 1 3 0
0 2 3 4 10
8 2 2 0 7
\end{pythonOutput}
\begin{pythonOutput}
./exercise2-1.py load array.txt 
[['0', '1', '1', '3', '0'], ['0', '2', '3', '4', '10'], ['8', '2', '2', '0', '7']]
\end{pythonOutput}
For the saving part we define a proof-of-concept array we can save to a file and get the following content:
\begin{pythonOutput}
./exercise2-1.py save test_array.txt 
1 2
2 3
3 4
\end{pythonOutput}

\section{Exercise 2.2}
Write a script that takes an integer N, and outputs all bit-strings of length N as lists.
\pythonexternal{"../Lesson 2/exercise2-2.py"}
\begin{pythonOutput}
./exercise2-1.py 3
[[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]]
\end{pythonOutput}
\section{Exercise 2.3}

\textbf{Write a script that takes this file (from this Kaggle competition), extracts the request\_text field from each dictionary in the list, and construct a bag of words representation of the string (string to count-list).}

There should be one row pr. text. The matrix should be N x M where N is the number of texts and M is the number of distinct words in all the texts.

\pythonexternal{"../Lesson 2/exercise2-3.py"}
We can then run the script on a test file with contents:
\begin{pythonOutput}
[
{"request_text":"this is a test question"},
{"request_text":"to be or not to be that is the question"}
]
\end{pythonOutput}
we then get the following output:
\begin{pythonOutput}
[1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0]
[0, 0, 1, 0, 2, 1, 1, 2, 1, 1, 1]
\end{pythonOutput}
\section{Exercise 3.1}
\textbf{Write a script which reads a matrix from a file like this one and solves the linear matrix equation Ax=b where b is the last column of the input-matrix and A is the other columns. It is okay to use the solve()-function from numpy.linalg.}
\pythonexternal{"../Lesson 3/exercise3-1.py"}
\begin{pythonOutput}
The answer is [-5.09090909  1.18181818  2.24242424]
\end{pythonOutput}
\section{Exercise 3.2}
\textbf{Write a script that reads in this list of points (x,y), fits/interpolates them with a polynomial of degree 3. Solve for the (real) roots of the polynomial numerically using Scipy’s optimization functions (not the root function in Numpy).}
\pythonexternal{"../Lesson 3/exercise3-2.py"}
We can see that the interpolation fits in \Cref{fig:interpolation_plot}.

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{"../Lesson 3/interpolation_plot"}
\caption{With a Figure caption}
\label{fig:interpolation_plot}
\end{figure}

We get the following root:
\begin{pythonOutput}
[-1.2114903]
\end{pythonOutput}
\section{Exercise 3.3}
\textbf{Do the first two exercises (Todo’s) at the bottom of http://byumcl.bitbucket.org/bootcamp2013/labs/pandas.html}
\pythonexternal{"../Lesson 3/exercise3-3.py"}
\begin{pythonOutput}
Loading data. Please wait...

Movies with the most number of ratings
American Beauty (1999)                                   3428
Star Wars: Episode IV - A New Hope (1977)                2991
Star Wars: Episode V - The Empire Strikes Back (1980)    2990
Star Wars: Episode VI - Return of the Jedi (1983)        2883
Jurassic Park (1993)                                     2672
Name: rating, dtype: int64


The 3 movies with the highest number average rating for females
Close Shave, A (1995)                                           4.644444
Wrong Trousers, The (1993)                                      4.588235
Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)                   4.572650
Name: rating, dtype: float64


The 3 movies with the highest number average rating for Males
Godfather, The (1972)                                           4.583333
Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)    4.576628
Shawshank Redemption, The (1994)                                4.560625
Name: rating, dtype: float64


The 10 movies men liked much more than women
Good, The Bad and The Ugly, The (1966)          0.726351
Kentucky Fried Movie, The (1977)                0.676359
Dumb & Dumber (1994)                            0.638608
Longest Day, The (1962)                         0.619682
Cable Guy, The (1996)                           0.613787
Evil Dead II (Dead By Dawn) (1987)              0.611985
Hidden, The (1987)                              0.607167
Rocky III (1982)                                0.581801
Caddyshack (1980)                               0.573602
For a Few Dollars More (1965)                   0.544704
Name: rating, dtype: float64


The 10 movies women liked much more than men
Dirty Dancing (1987)                           -0.830782
Jumpin' Jack Flash (1986)                      -0.676359
Grease (1978)                                  -0.608224
Little Women (1994)                            -0.548849
Steel Magnolias (1989)                         -0.535777
Anastasia (1997)                               -0.518391
Rocky Horror Picture Show, The (1975)          -0.512885
Color Purple, The (1985)                       -0.498851
Age of Innocence, The (1993)                   -0.487561
Free Willy (1993)                              -0.482573
Name: rating, dtype: float64


Movies with the highest rating standard deviation
Dumb & Dumber (1994)                                 1.321333
Blair Witch Project, The (1999)                      1.316368
Natural Born Killers (1994)                          1.307198
Tank Girl (1995)                                     1.277695
Rocky Horror Picture Show, The (1975)                1.260177

\end{pythonOutput}

\section{Exercise 3.4}
\textbf{Last week you read in a dataset for this Kaggle competition and created a bag-of-words representation on the review strings. Train a logistic regression classifier for the competition using your bag-of-words features (and possibly some of the others) to predict the variable “requester\_received\_pizza”. For this exercise, you might want to work a little bit more on your code from last week. Use 90\% of the data as training data and 10\% as test data. How good is your classifier? Discuss the performance of the classifier.}
\pythonexternal{"../Lesson 3/exercise3-4.py"}

%By just using a BoW representation of the request_text, we can predict the accuracy by doing a sum(logistic.predict(X_test) == y_test)/len(y_test) or we can use the logistic regression score method, we get the following score:

\begin{pythonOutput}
0.683168316832
\end{pythonOutput}
which is 68.31\% predicted correctly
If we add number of posts at retrieval, which might very well be correlated with a request being fulfilled we get the following score:
\begin{pythonOutput}
0.777227722772
\end{pythonOutput}
so 77.72\% an increase of 9.41\% by adding that extra feature.
\section{Exercise 3.5}

\textbf{Write a simple Python function for computing the sum $\frac{1}{1^2} + \frac{1}{2^2} + \frac{1}{3^2} + \ldots$ with 10,000 terms (this should be around 1.644), 500 times in a row (to make the execution time measurable). Now compile the code with Cython and see how much speedup you can achieve by this.}
We run the script on a Intel i5 2500K processor,
First we create a python version of the function:
\pythonexternal{"../Lesson 3/exercise3_5_python.py"}
\begin{pythonOutput}
1.017233310994925
\end{pythonOutput}
Thus time taken with 500 executions is 1.0172 seconds.

Next we create a cython version, we use two files for this, one importing pyximport and running it from python and one with the actual cython code being run:
\pythonexternal{"../Lesson 3/run_exercise3_5_cython.py"}
\pythonexternal{"../Lesson 3/exercise3_5_cython.pyx"}
\begin{pythonOutput}
0.03667000599671155
\end{pythonOutput}
and time taken with 500 executions for the cython function is 0.0366\\
Thus we get a speedup of 27.74x, not too shabby
\section{Exercise 4.1}
\textbf{Implement the DBSCAN clustering algorithm to work with Jaccard-distance as its metric. It should be able to handle sparse data.}
We implemented the DBSCAN algorithm using a combination of Numpy and Cython. We used memoization for the distances and thus store them in distance_matrix as soon as we compute them to avoid unnecessary calls to the jaccard distance method.
The memoization of distances will not work for the 100000x100000 dataset due memory size constraints, thus the execution for that one is without memoization.
\pythonexternal{"../Lesson 4/run_dbscan_cython.py"}
\pythonexternal{"../Lesson 4/dbscan_cython.pyx"}
\begin{table}[]
\centering
\caption{Performance table for DBSCAN}
\label{my-label}
\begin{tabular}{llll}
\textbf{Dimensions}                          & \textbf{time in seconds} & \textbf{Number of clusters} & \textbf{Number of points in largest cluster} \\
10x10                                        & 0.0005862309990334325    & 4                           & 4                                            \\
100x100                                      & 0.003060722003283445     & 6                           & 30                                           \\
1000x1000                                    & 0.09427501499885693      & 9                           & 289                                          \\
10000x10000                                  & 10.893330368002353       & 394                         & 2847                                         \\
100000x100000 (without distance memoization) & 4475.570929656002        & 1692                        & 28470                                       
\end{tabular}
\end{table}


\end{document}
