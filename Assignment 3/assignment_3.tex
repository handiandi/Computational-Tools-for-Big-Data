\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage{amsmath,amssymb,graphicx}
\usepackage{cleveref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[export]{adjustbox}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal
\DeclareUnicodeCharacter{00D6}{Ö}
\DeclareUnicodeCharacter{00DC}{Ü}
\DeclareUnicodeCharacter{00E4}{ä}
\DeclareUnicodeCharacter{00E4}{ä}

% Custom colors
\usepackage{color}

\definecolor{output_background}{rgb}{0.2, 0.2, 0.2}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}
\definecolor{purple}{rgb}{0.6, 0.1, 0.9}
\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=python,
breaklines=true,
basicstyle=\ttfamily\small,
otherkeywords={1, 2, 3, 4, 5, 6, 7, 8 ,9 , 0, -, =, +, [, ], (, \), \{, \}, :, *, !},             % Add keywords here
keywordstyle=\color{blue},
emph={class, pass, in, for, while, if, is, elif, else, not, and, or, OR
    def, print, exec, break, continue, return},
emphstyle=\color{black}\bfseries,
emph={[2]True, False, None, self},
emphstyle=[2]\color{purple},
emph={[3]from, import, as},
emphstyle=[3]\color{blue},
stringstyle=\color{red},
frame=tb,
showstringspaces=false,
morecomment=[s]{"""}{"""},
commentstyle=\color{gray},
rulesepcolor=\color{blue},
title=\lstname
}}

% Python style for output highlighting
\newcommand\pythonoutputstyle{\lstset{
backgroundcolor=\color{output_background},
rulecolor=\color{output_background},
basicstyle=\ttm\small\color{white},
showstringspaces=false,
inputencoding=utf8,
extendedchars=true,
literate=%
    {á}{{\'a}}1
    {č}{{\v{c}}}1
    {ď}{{\v{d}}}1
    {é}{{\'e}}1
    {ě}{{\v{e}}}1
    {í}{{\'i}}1
    {ň}{{\v{n}}}1
    {ó}{{\'o}}1
    {ř}{{\v{r}}}1
    {š}{{\v{s}}}1
    {ť}{{\v{t}}}1
    {ú}{{\'u}}1
    {ů}{{\r{u}}}1
    {ý}{{\'y}}1
    {ž}{{\v{z}}}1
    {Á}{{\'A}}1
    {Č}{{\v{C}}}1
    {Ď}{{\v{D}}}1
    {É}{{\'E}}1
    {Ě}{{\v{E}}}1
    {Í}{{\'I}}1
    {Ň}{{\v{N}}}1
    {Ó}{{\'O}}1
    {Ř}{{\v{R}}}1
    {Š}{{\v{S}}}1
    {Ť}{{\v{T}}}1
    {Ú}{{\'U}}1
    {Ů}{{\r{U}}}1
    {Ý}{{\'Y}}1
    {Ž}{{\v{Z}}}1
    {Ö}{{\"{O}}}1
    {ö}{{\"{o}}}1
    {Ü}{{\"{U}}}1
    {ü}{{\"{u}}}1   
    {ß}{{\ss}}1
    {ä}{{\"{a}}}1
    {é}{{\'{e}}}1
    {ô}{{\^{o}}}1
    {â}{{\^{a}}}1
    {è}{{\`{e}}}1
}}


\lstnewenvironment{pythonOutput}[1][]
{
\pythonoutputstyle
\lstset{#1}
}
{}

% Python for inline
\newcommand\pythonoutput[1]{{\pythonoutputstyle\lstinline!#1!}}


% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

\title{Assignment 3\\02807 Computational Tools for Big Data}
\author{S \& A}
\date{26th October 2015}
\usepackage[cm]{fullpage}
\begin{document}

\maketitle
\newpage
%-------------- Week 5 ---------------------
\section{Exercise 8.1}
\textbf{Short recap of the exercise}\\
\textit{Define and implement a MapReduce job to count the occurrences of each word in a text file. Document that it works with a small example.}\\
~\\
\textbf{Overview}\\
We implement the MapReduce jobs by creating mappers and reducers. A mapper takes key/value pairs and generates key/value pairs, a reducer iterates through values for each distinct key, and generates an output.\\\\
For this exercise, the mapper takes a line as value, then splits them and yields each word and the count 1. The reducer sums the value for each unique key and yields the result. 
\\*
~\\
\textbf{Code}
\pythonexternal{"../Lesson 8/exercise-8-1.py"}
~\\
\textbf{Input}
\begin{pythonOutput}
der var engang en mand
der boede i en spand
spanden var af ler
nu kan jeg ikke mer
\end{pythonOutput}
\textbf{Output}
\begin{pythonOutput}
"af"    1
"boede"    1
"der"   2
"en"    2
"engang"  1
"i" 1
"ikke"  1
"jeg"   1
"kan"   1
"ler"   1
"mand"  1
"mer"   1
"nu"    1
"spand" 1
"spanden"   1
"var"   2
\end{pythonOutput}
\textbf{Explanation of output and discussion}\\
We can see it returns the key (word) and the sum of the ones, thus the word count.~\\

\section{Exercise 8.2}
\textbf{Short recap of the exercise}\\
\textit{Define and implement a MapReduce job that determines if a graph has an Euler tour (all vertices have even degree) where you can assume that the graph you get is connected.
It is fine if you split the file into five different files. You do not need to keep the node and edge counts in the top of the file.}\\
~\\
\textbf{Overview}\\
For this exercise we use one mapper and two reducers.~\\
The steps method tells MrJob in which order the methods should run. The order of execution is mapper, reducer, reducer2.

\begin{itemize}
  \item \textit{mapper} - takes each line and split it into from- and to-edges. Then it yields these two as (key,1) pairs
  \item \textit{reducer} - sums the values of the key, as we did in exercise 8.1
  \item \textit{reducer2} - checks if all values of the key is even\\
\end{itemize}
~\\
We  split the input file in five files and get a concatenation of outputs with the following bash command: ~\\
ls graph*.txt | xargs -n1 ./exercise-2.py -q \\
~\\
This runs the script with all graph files as arguments in ascending order.

~\\
\textbf{Code}
\pythonexternal{"../Lesson 8/exercise-2.py"}
~\\
\textbf{Output}
\begin{pythonOutput}
"Has Euler Tour"    true
"Has Euler Tour"    false
"Has Euler Tour"    true
"Has Euler Tour"    true
"Has Euler Tour"    false

\end{pythonOutput}


\section{Exercise 8.3}
\textbf{Short recap of the exercise}\\
\textit{Implement the MapReduce job from the lecture which finds common friends (note that for the Facebook file, you need to extend the job to convert from a list of edges to the format from the slides – do this with an additional map/reduce job).}\\
~\\
\textbf{Overview}\\
We split this assignment in two: one for the example in the slides, and one for the facebook file. ~\\
\textbf{For the slides example}\\
For finding common friends of the slides we use a \textit{mapper} and \textit{reducer} drawing inspiration from the Lecture 8 slides\footnote{\url{https://www.dropbox.com/s/xogs1ozzq7aqbod/mapreduce.pdf?dl=0}}:
\begin{itemize}
\item \textit{mapper} - converts to sorted node pairs and their common friends, then we create two groups based on node pair key.
\item \textit{reducer} - reduce by yielding the intersection of the two groups.
\end{itemize}
The functionality of the \textit{main\_mapper} and \textit{main\_reducer} are similar to the previous exercise.
~\\
\textbf{For the facebook file}
For finding common friends of the Facebook file we convert from a list of edges to the slides format. This is done by the \textit{convert\_mapper} and \textit{convert\_reducer}:
\begin{itemize}
\item \textit{convert\_mapper} - simply duplicates the to- and -from edge in (to,from) and (from, to) pairs.
\item \textit{convert\_reducer} - reduce by key, creating a tuple of neighbor nodes.
\end{itemize}
The functionality of the \textit{main\_mapper} and \textit{main\_reducer} are similar to the previous exercise.
~\\
\textbf{Code - Slides example}
\pythonexternal{"../Lesson 8/exercise-3-1.py"}
\textbf{Output - Slides example}
\begin{pythonOutput}
["A", "B"]  ["C", "D"]
["A", "C"]  ["B", "D"]
["A", "D"]  ["B", "C"]
["B", "C"]  ["A", "D", "E"]
["B", "D"]  ["A", "C", "E"]
["B", "E"]  ["C", "D"]
["C", "D"]  ["A", "B", "E"]
["C", "E"]  ["B", "D"]
["D", "E"]  ["B", "C"]


\end{pythonOutput}
\textbf{Code - Facebook file}
\pythonexternal{"../Lesson 8/exercise-3-2.py"}
\textbf{Output - Facebook file}
\begin{pythonOutput}
[0, 100]    [119, 150, 163, 189, 217, 269, 323, 64]
[0, 101]    [180, 187, 194, 204, 24, 242, 249, 254, 266, 299, 302, 317, 330, 346, 53, 80, 92, 
             94]
[0, 102]    [175, 227, 263, 296, 99]
[0, 103]    [136, 169, 172, 185, 200, 211, 25, 252, 271, 285, 323, 339, 56, 7, 98]
[0, 104]    [109, 113, 122, 123, 128, 142, 169, 186, 188, 200, 203, 21, 212, 239, 25, 252, 26, 
             271, 277, 295, 303, 318, 322, 325, 332, 344, 45, 55, 56, 67, 98]
[0, 105]    [119, 148, 21, 236, 25, 257, 272, 277, 280, 315, 39, 69, 9]
[0, 106]    [169, 231, 238, 29, 329, 332, 88]
[0, 107]    [171, 58]
[0, 108]    [127, 159, 184, 197, 21, 251, 272, 281, 284, 320, 36, 57]
[0, 109]    [104, 118, 119, 122, 13, 142, 148, 158, 169, 186, 200, 203, 21, 229, 239, 252, 26, 
             271, 277, 285, 295, 297, 303, 304, 31, 314, 322, 323, 324, 325, 331, 332, 50, 56, 
             67, 98]


\end{pythonOutput}


\section{Exercise 8.4}
\textbf{Short recap of the exercise}\\
\textit{Make a MapReduce job which counts the number of triangles in a graph.}\\
~\\
\textbf{Overview}\\
The convert\_mapper, convert\_reducer and main\_mapper steps are the same as in exercise 8.3. ~\\
The other steps are:\\
 ~\\
 \begin{itemize}
\item \textit{main\_reducer} -  finds common "friends" (nodes) of its key-values. The new key is the original key and the new friend ( [old\_key, friend] ). It yields the new key and the value 1.
\item \textit{sum\_reducer} - to reduce by key and sums the value.
\item \textit{count\_triangle\_mapper} - checks if both length of key is 3 and value is 3. If it is, it yield 1 as value, else 0. The key is the summation of what we wil find: "Number of Triangles"
\item \textit{sum\_triangle\_reducer} - to reduce by key and sum the value, which gives us the final result.
\end{itemize}
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 8/exercise-8-4.py"}
~\\
\textbf{Output}
\begin{pythonOutput}
./exercise-8-4-v2.py 8.4_small_test.txt -q
"Number of Triangles"   120676
\end{pythonOutput}
\textbf{Explanation of output and discussion}\\
We can see we found 120676 triangles in the graph of the road network of California.~\\
From their website\footnote{\url{http://www.cise.ufl.edu/research/sparse/matrices/SNAP/roadNet-CA.html}} we can see their number of triangles found corresponds with ours. 


%-------------- Week 9 ---------------------
\section{Exercise 9.1}
\textbf{Short recap of the exercise}\\
\textit{Write a Spark job to count the occurrences of each word in a text file. Document that it works with a small example}\\
~\\
\textbf{Overview}
~\\
Following is a summarization of the comments in the file. 
\begin{enumerate}
  \item Extract lines in the file
  \item Splitting the lines into words
  \item Creating a pair for each word in the form ( word , count ) where count is the occurrence of the word , set to 1
  \item Counting the words
  \item Get the result and sort it based on count
  \item Print out the result\\
\end{enumerate}
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 9/exercise-9-1.py"}~\\
\textbf{Output}~\\
\begin{pythonOutput}
'en' has an occurrences of 3
'der' has an occurrences of 2
'var' has an occurrences of 2
'spand' has an occurrences of 1
'spanden' has an occurrences of 1
'af' has an occurrences of 1
'i' has an occurrences of 1
'kan' has an occurrences of 1
'booede' has an occurrences of 1
'ikke' has an occurrences of 1
'ler' has an occurrences of 1
'mand' has an occurrences of 1
'gang' has an occurrences of 1
'mer' has an occurrences of 1
'nu' has an occurrences of 1
'jeg' has an occurrences of 1
\end{pythonOutput}
~\\
\textbf{Explanation of output and discussion}\\
We can see that the output is identical with exercise 8.1, and since we have validated the result i 8.1, this result is also correct. 

\section{Exercise 9.2}
\textbf{Short recap of the exercise}\\
\textit{Write a Spark job that determines if a graph has an Euler tour (all vertices have even degree) where you can assume that the graph you get is connected.~\\
It is fine if you split the file into 5 different files. You do not need to keep the node and edge counts in the top of the file.}\\
~\\
\textbf{Overview}
As in the exercise in Week 8, we have splittet the file into 5 different files. ~\\
We have created a python function 'has\_euler\_tour' which take a filename string as argument. \\
~\\
The following is a step-by-step explaination of the code (primarily the function). 
\begin{enumerate}
  \item Loading in the file
  \item Creating a pair of integer values in the form (from\_node , to\_node), by splitting on whitespace
  \item Creating a list of pairs in the form [(from\_node , to\_node), (to\_node , from\_node)]
  \item Creating a pair for each node in position 1 of the pairs in the list. The new pair has the form (node, count) where count is the occurrence of the node, set to 1
  \item We use reduceByKey to summerize the values. reduceByKey returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function a+b. 
  \item filter-function find those whos values are not even
  \item If the count of result from filter-function is 0, return true, else return false 
  \item Print out the result\\
\end{enumerate}
What the code actually does, is to count the edges for each node. When we create the list of opposite pairs (step 3), we account for both to and from edges, which mean we can simply count the occurences of the node in positon 1 (step 4). ~\\
From this we can decide whether it has Euler tour. 
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 9/exercise-9-2.py"}~\\
\textbf{Output}~\\
\begin{pythonOutput}
Has Euler Tour: True
Has Euler Tour: False
Has Euler Tour: True
Has Euler Tour: True
Has Euler Tour: False
\end{pythonOutput}
~\\
\textbf{Explanation of output and discussion}\\
We can see that the output is identical with exercise 8.2, so we assume this result is correct. 

 ~\\

\section{Exercise 9.3}
\textbf{Short recap of the exercise}\\

\textit{Compute the following things using Spark:
\begin{enumerate}
    \item What are the 10 networks I observed the most, and how many times were they observed? Note: the bssid is unique for every network, the name (ssid) of the network is not necessarily unique.
    \item What are the 10 most common wifi names? (ssid)
    \item What are the 10 longest wifi names? (again, ssid)
\end{enumerate}}
\textbf{Overview}~\\
We have created a single file with the code for all three tasks, for each task we will explain the relevant functions used if they differ from the previous task. ~\\

\textbf{What are the 10 networks I observed the most, and how many times were they observed?}~\\
We load the wifi.data file, then we extract bssid from it by converting it to a dict usin the eval() function. We make a pair (bssid, count) where count is 1. Then we add the counts together with reduceByKey and print out top 10 in descending order (by count). ~\\

 \textbf{What are the 10 most common wifi names? (SSID)}~\\
We extract distinctive pairs of SSIDs and BSSID using the distinct function, but omit duplicate access points.
We then count the SSIDs and print them like in the previous exercise. ~\\

 \textbf{What are the 10 longest wifi names? (SSID)}~\\
We get the distinct SSID, Then we print out top 10 in descending order by using the negative length of the SSID as the key. ~\\~\\ 
\textbf{Code}
\pythonexternal{"../Lesson 9/exercise-9-3.py"}~\\
\textbf{Output - What are the 10 networks I observed the most, and how many times were they observed? Note: the BSSID is unique for every network, the name (SSID) of the network is not necessarily unique.}
\begin{pythonOutput}
[(u'34:21:09:12:6c:1a', 347),
 (u'00:24:b2:98:39:d2', 338),
 (u'34:21:09:12:6c:18', 324),
 (u'e8:08:8b:c9:c1:79', 318),
 (u'44:94:fc:56:08:fb', 315),
 (u'00:22:b0:b3:f2:ea', 314),
 (u'2c:b0:5d:ef:08:2b', 272),
 (u'44:94:fc:56:ce:5e', 240),
 (u'28:cf:e9:84:a1:c3', 211),
 (u'bc:ee:7b:55:1a:43', 210)]
\end{pythonOutput}
\textbf{Output - What are the 10 most common wifi names? (SSID)}
\begin{pythonOutput}
[(u'', 15),
 (u'SIT-GUEST', 15),
 (u'SIT-BYOD', 13),
 (u'SIT-PROD', 13),
 (u'PDA-105', 11),
 (u'KNS-105', 11),
 (u'MED-105', 11),
 (u'KEA-PUBLIC', 6),
 (u'GST-105', 5),
 (u'wireless', 4)]
\end{pythonOutput}
\textbf{Output - What are the 10 longest wifi names? (SSID)}
\begin{pythonOutput}
[u'HP-Print-43-Deskjet 3520 series',
 u'TeliaGatewayA4-B1-E9-2C-9E-CA',
 u'TeliaGateway08-76-FF-84-FF-8C',
 u'TeliaGateway9C-97-26-57-15-F9',
 u'TeliaGateway08-76-FF-46-3E-36',
 u'TeliaGateway9C-97-26-57-15-99',
 u'TeliaGateway08-76-FF-8A-EE-32',
 u'TeliaGateway08-76-FF-85-04-2F',
 u'TeliaGateway08-76-FF-9C-E0-82',
 u'Charlotte R.s Wi-Fi-netv\xe6rk']

\end{pythonOutput}
\textbf{Explanation of code and discussion}
From the outputs we can see that the network with BSSID 34:21:09:12:6c:1a was observed the most, the most common wifi name was empty string and SIT-GUEST and the longest wifi name was HP-Print-43-Deskjet 3520 series.


%-------------- Week 10 ---------------------
\section{Exercise 10.1}
\textbf{Short recap of the exercise}\\
\textit{Explain in your own words what Deep Learning is.}~\\
\textbf{Answer}\\
Deep Learning (DL) is a subset/branch of Machine Learning (which is the study of algorithms which can improve themselves over time).\\

DL is used as any other Machine Learning technique, to learn from data, and predict an outcome based on the trained model. DL is based on Artificial Neural Networks (ANN - a Machine Learning model), an ANN is a machine learning model inspired by the neural network of the brain, it consists of a large number of interconnected weighted neurons which can activate each other and the weights can be tuned which makes it adaptable and able to learn. The difference between ANN and DL is, that DL trains multi-layered or hierarchical ANN's.\\

A DL model is able to use a technique called representational learning to learn features of the dataset, thus when you give it a raw representation of the data (that is human interpretable), it learns itself what the different features are and it's then able to do recognition/supervised learning.\\

The ANN's each learns a feature through a nonlinear transformation and pass it along to the next ANN in the hierarchy until it gets to a classifier. Through each ANN the features learned are higher representations of the data. For example for facial recognition, for the first ANN receiving the input data, the feature learned is edges, then the next ANN learns different combinations of edges forming a part of a face, and so on.\\

Deep learning is very effective dealing with unstructured data like media (sounds, images, videos, text, time-series data)\\

It can be used for recommender systems and thus try to predict a rating or peference an user would give to an item, this is used currently for media streaming services like Spotify or Netflix, which can recommend new music or shows based on your playlist.\\

\section{Exercise 10.2}
\textbf{Short recap of the exercise}\\
\textit{Explain in your own words what a Convolutional Neural Network is.}\\
~\\
\textbf{Answer}\\
A Convolutional Neural Network (CNN) is a variant of the Artificial Neural Network (ANN). It is inspired by the human visual system and is typically used in image recognition.\\

It works similarly to a (ANN) by using layers of neurons where each layer feeds into the next layer and for each layer, the layer will obtain higher-level features.\\

However where in an ANN each neuron in a given layer feeds into all the neurons in the next layer, in a CNN a subset or "patch" of neurons feeds into a number of neurons in the next layer, that patch of neurons is called the local receptive field, for image recognition, this means small "overlapping windows" of the input image is building the next layer resulting in a slightly smaller layer than the former. In a CNN each layer will have the same weights and bias and thus detect the same feature just at different windows in the image. Using the same weights and bias reduce the parameters involved and computational power needed.\footnote{\url{http://neuralnetworksanddeeplearning.com/chap6.html}}\\

A CNN often makes use of a max-pooling layer, in between convolutional layers, which takes a maximum of the features over segments of the input layer, to reduce the spatial size of the data thus reducing the amount of parameters, and makes the model invariant to very small transformations of the data (overfitting).\footnote{\url{http://colah.github.io/posts/2014-07-Conv-Nets-Modular/}}.\\

With neural network (and other commonly machine learning techniques), feature engineering is one of the most essential and important task for good classification\footnote{\url{http://blog.kaggle.com/2014/08/01/learning-from-the-best/}}. Feature engineering is a very hard and complex task, which you would have to do for each type of problems. Using CNN, feature engineering is beeing done when we train it. 
For each training, it gets better and better to filter the input for relevant information (feature engineering). Once we learned our hierarchical features, we can simply pass them to a fully connected, simple neural network that combines them in order to classify the input into classes\footnote{\url{http://timdettmers.com/2015/03/26/convolution-deep-learning/}}.\\

\section{Exercise 10.3}
\textbf{Short recap of the exercise}\\
\textit{In this exercise you will be using the Caffe Deep Learning framework to classify a picture of a cat, and a picture of your own choosing.}\\
~\\
\textbf{Overview}\\
We started the virtual machine by running \textit{vagrant up} and navigating via our browser to \textit{localhost:8003}\\
We executed the code contained in the Jupyter notebook with a series of pictures: the default cat picture, a picture of a pig as seen in Figure \ref{fig:pig}, a picture of a beer glass \ref{fig:beer_glass}, a picture of a cloud and below each picture we got an output.
~\\
\textbf{Output - cat}\\
\begin{pythonOutput}
Predicted class is #282.
['n02123159 tiger cat' 'n02123045 tabby, tabby cat'
 'n02124075 Egyptian cat' 'n02119022 red fox, Vulpes vulpes'
 'n02127052 lynx, catamount']
\end{pythonOutput}
\textbf{Output - pig}
\begin{figure}[h!]
\begin{center}
\caption{Picture of a pig.}
\label{fig:pig}
\includegraphics[scale=0.5]{"../Lesson 10/images/pig"}
\end{center}
\end{figure}
\begin{pythonOutput}
Predicted class is #341.
['n02395406 hog, pig, grunter, squealer, Sus scrofa'
 'n01632777 axolotl, mud puppy, Ambystoma mexicanum'
 'n03223299 doormat, welcome mat' 'n02328150 Angora, Angora rabbit'
 'n02412080 ram, tup']
\end{pythonOutput}
\textbf{Output - beer glass}
\begin{figure}[h!]
\begin{center}
\caption{Picture of a beer glass.}
\label{fig:beer_glass}
\includegraphics[scale=0.25]{"../Lesson 10/images/beer_glass"}
\end{center}
\end{figure}
\begin{pythonOutput}
Predicted class is #855.
['n04423845 thimble' 'n02823750 beer glass'
 'n07615774 ice lolly, lolly, lollipop, popsicle'
 'n04131690 saltshaker, salt shaker' 'n02948072 candle, taper, wax light']
\end{pythonOutput}

\textbf{Output - cloud}
\begin{figure}[h!]
\begin{center}
\caption{Picture of cloud.}
\label{fig:cloud}
\includegraphics[scale=0.5]{"../Lesson 10/images/cloud"}
\end{center}
\end{figure}
\begin{pythonOutput}
Predicted class is #190.
['n02095889 Sealyham terrier, Sealyham' 'n04266014 space shuttle'
 'n09288635 geyser' 'n02948072 candle, taper, wax light' 'n03825788 nipple']
\end{pythonOutput}
\textbf{Explanation of code and discussion}\\

The predicted class for the default cat picture results in a correct classification as "tabby" or "tabby cat" among the top 4 answers, we then tried the pig picture in Figure \ref{fig:pig} which we can read resulted in the correct classification as hog/pig. Next we tried somewhat to trick it by showing it an alternatively shaped beer glass as shown in Figure \ref{fig:beer_glass}, this was also correctly identified in the second answer, next we tried a cloud as seen in Figure \ref{fig:cloud}, and it seemed to do the trick as none of the answers given by the model were correct.
%-------------- Week 11 ---------------------
\section{Exercise 11.1}
\textbf{Short recap of the exercise}\\
\textit{Train a random forest classifier to predict the topic earn in the articles, then implement it using feature hashing with 1000 buckets}\\
~\\
\textbf{Overview}
\textbf{Code}\\
\pythonexternal{"../Lesson 11/exercise_11_1.py"}
~\\
\textbf{Output}
\begin{pythonOutput}
Accuracy using a bag-of-words representation: 
(10377, 70793)
0.958092485549
Accuracy using feature-hashing with 2 buckets:
(10377, 2)
0.721579961464
Accuracy using feature-hashing with 10 buckets:
(10377, 10)
0.874759152216
Accuracy using feature-hashing with 100 buckets:
(10377, 100)
0.926782273603
Accuracy using feature-hashing with 500 buckets:
(10377, 500)
0.933526011561
Accuracy using feature-hashing with 1000 buckets:
(10377, 1000)
0.939306358382


\end{pythonOutput}
\textbf{Explanation of code and discussion}\\
We implemented feature-hashing using the Exercise description and the Lesson 11 slides\footnote{\url{https://www.dropbox.com/s/8vh8nyywttf5y6e/hashing.pdf?dl=0}}.\\
We created the bag-of-words representation with a custom tokenizer that split the strings and lowercased the tokens.\\
In the articles dataset we found that 3776 articles contained the topic earn out of the total of 10377 articles, by simply guessing this would mean an accuracy of 63.611\%. In the output we can see the shape and accuracies of feature-hashing with a number of buckets. We find that even with only 2 buckets, the accuracy is 72.157\%, and for a bigger number of buckets the accuracy is nearing the accuracy of the bag-of-words representation. For the bag-of-words representation with 70793 features the accuracy is 95.809\% and for feature-hashing with 1000 buckets it is 93.930\%.

\section{Exercise 11.2}
\textbf{Short recap of the exercise}\\
\textit{Implement your own MinHash algorithm. Try with different number of hash functions/permutations (for example 3, 5, 10).
Look at which documents end up in the same buckets. Do they look similar? Do they share the same topics?}\\
~\\
\textbf{Overview}\\
See our description after the sourcecode and outputs\\
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 11/exercise_11_2.py"}
~\\
\textbf{Output - with permutations = 3}\\
\begin{pythonOutput}
#### Summary ####
There are 8 buckets with the following:
----------------------
Number of articles: 1
Topic count: Counter({'interest': 1})

Number of articles: 1
Topic count: Counter({'acq': 1})

Number of articles: 2
Topic count: Counter({'money-fx': 1, 'earn': 1, 'interest': 1})

Number of articles: 2
Topic count: Counter({'acq': 2})

Number of articles: 91
Topic count: Counter({'earn': 41, 'acq': 19, 'crude': 6, 'grain': 5, 'corn': 3, 'money-fx': 3, 'money-supply': 2, 'nat-gas': 2, 'gnp': 2, 'cpi': 2, 'silver': 2, 'interest': 2, 'livestock': 2, 'copper': 2, 'zinc': 2, 'sugar': 2, 'wheat': 2, 'strategic-metal': 1, 'pork-belly': 1, 'coffee': 1, 'gold': 1, 'lei': 1, 'orange': 1, 'hog': 1, 'reserves': 1, 'yen': 1, 'lead': 1, 'nzdlr': 1})

Number of articles: 1
Topic count: Counter({'interest': 1, 'money-fx': 1})

Number of articles: 1
Topic count: Counter({'crude': 1, 'ship': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

#################

#### Printing 3 random articles from buckets which have more than 3 articles ####
----------------------

-----------------------
Bucket with 91 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Lennar Corp chairman and president,
Leonard Miller, said the current backlog of orders and the
strong economy point to strong revenues and earnings for the
balance of fiscal 1987.
    He said the company's backlog of sales deposits on Feb 28
was 2,416, an increase of 976 units over the previous year.
    Lennar recorded net earnings for the first quarter 1987 of
4,403,000, or 51 cts per share, compared to 1,775,000, or 20
cts per share the prior first quarter. It recorded net earnings
of 12.5 mln dlrs, or 1.43 dlrs per share, for fiscal 1986.
    The company also said that at its April 29 annual meeting,
shareholders will vote on increasing the company's authorized
common stock to 45 mln shares from 15 mln. This will include 30
mln shares of common stock and 15 mln shares of class B common
stock, it added.
    Those shareholders who elect to convert their shares into
class B stock will be entitled to 10 votes per share while
other shareholders will retain one vote per share, Lennar said.
    The company said if this is approved, it intneds to pay
holders of Class B stock a quarterly cash dividend of five cts
per share and holders of the other common stock a quarterly
cash dividend of six cts per share.
 Reuter

['earn']

XXXXXXXXXX
New article
XXXXXXXXXX

ChemLawn Corp said it has
started talks on the possible sale of the company with "various
parties" that it did not identify.
    The company said the talks began after it considred and
rejected Waste Management Inc's <WMX> 27 dlr per share tender
offer for all its sharesd.
    ChemLawn gave no details on the talks.
 Reuter

['acq']

XXXXXXXXXX
New article
XXXXXXXXXX

Unocal Corp said it intends to
increase its spending for capital projects to 929 mln dlrs in
1987, eight pct more than the 862 mln spent in 1986.
    The company said in its annual report that it would
increase spending for exploration and development of petroleum
resources by about three pct to 614 mln dlrs from 1986's 595
mln dlrs, assuming oil prices hold around current levels.
    The planned spending for exploration and production in 1987
remains well below the 1.1 billion dlrs spent in 1985, Unocal
said.
    The company's proved developed and undeveloped reserves of
crude oil rose slightly in 1986, Unocal said. Net crude oil and
condensate reserves were 752 mln bbls as of Dec 31, 1986,
compared to 751 mln bbls at the end of 1985, Unocal said.
    The company said its net crude oil and condensate
production averaged 248,200 barrels per day in 1986 compared to
251,300 bpd in 1985.
    Unocal said its worldwide natural gas reserves were 6.07
billion cubic feet in 1986 compared to 1985's 6.19 billion. Net
natural gas output averaged 976 mln cubic feet per day in 1986,
down 10 pct from 1985's 1,084 mln, the company said.
    Unocal said its average sales prices for crude oil was
12.67 dlrs a barrel worldwide in 1986 compared to 23.81 dlrs in
1985, and its average sales price for natural gas was 2.03 dlrs
per thousand cubic feet in 1986 against 2.24 dlrs in 1985.
    Average production costs for crude oil and natural gas
declined nearly 30 pct to 3.41 dlrs per bbl of oil equivalent
in 1986 from 4.81 dlrs in 1985, Unocal said.
    In the annual report, the company called for imposition of
an oil import fee by the U.S. government to set a floor price
of about 25 dlrs a barrel for crude oil.
    "Simply stabilizing prices at about 18 dlrs per barrel    
will not materialy slow the drop in U.S. production or the rise
in imports," Chairman Fred Hartley said in the annual report.
    "Without decisive action in Washington, this nation will
once again become a hostage to OPEC's plans and policies,"
Hartley said.
 Reuter

['crude', 'nat-gas']


\end{pythonOutput}
\textbf{Output - with permutations = 5}\\
\begin{pythonOutput}
#### Summary ####
There are 39 buckets with the following:
----------------------
Number of articles: 2
Topic count: Counter({'acq': 1, 'earn': 1})

Number of articles: 1
Topic count: Counter({'crude': 1, 'nat-gas': 1})

Number of articles: 1
Topic count: Counter({'hog': 1, 'livestock': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 3
Topic count: Counter({'earn': 2, 'acq': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 5
Topic count: Counter({'acq': 3, 'crude': 1, 'money-fx': 1, 'nzdlr': 1})

Number of articles: 25
Topic count: Counter({'earn': 18, 'interest': 1, 'wheat': 1, 'crude': 1, 'money-supply': 1, 'corn': 1, 'acq': 1, 'money-fx': 1, 'coffee': 1})

Number of articles: 2
Topic count: Counter({'earn': 2})

Number of articles: 1
Topic count: Counter({'grain': 1})

Number of articles: 1
Topic count: Counter({'money-supply': 1})

Number of articles: 1
Topic count: Counter({'grain': 1})

Number of articles: 1
Topic count: Counter({'cpi': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 2
Topic count: Counter({'grain': 1, 'acq': 1, 'corn': 1, 'strategic-metal': 1})

Number of articles: 1
Topic count: Counter({'cpi': 1})

Number of articles: 1
Topic count: Counter({'sugar': 1})

Number of articles: 1
Topic count: Counter({'livestock': 1, 'pork-belly': 1})

Number of articles: 1
Topic count: Counter({'acq': 1})

Number of articles: 1
Topic count: Counter({'acq': 1})

Number of articles: 25
Topic count: Counter({'acq': 10, 'earn': 6, 'money-fx': 3, 'interest': 2, 'crude': 2, 'grain': 1, 'gnp': 1, 'gold': 1, 'sugar': 1, 'orange': 1, 'lead': 1, 'zinc': 1, 'wheat': 1, 'silver': 1, 'lei': 1})

Number of articles: 3
Topic count: Counter({'ship': 1, 'silver': 1, 'crude': 1, 'acq': 1, 'copper': 1, 'zinc': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'reserves': 1})

Number of articles: 1
Topic count: Counter({'crude': 1, 'nat-gas': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'interest': 1})

Number of articles: 1
Topic count: Counter({'interest': 1, 'gnp': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'yen': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'grain': 1, 'corn': 1})

Number of articles: 1
Topic count: Counter({'copper': 1, 'earn': 1})

Number of articles: 3
Topic count: Counter({'acq': 2, 'earn': 1})

#################

#### Printing 3 random articles from buckets which have more than 3 articles ####
----------------------

-----------------------
Bucket with 5 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Progressive Savings and Loan
Association <PRSL> said it has agreed in principle to be
purchased by Far West Financial Corp's Far West Savings and
Loan Association.
    The acquisition would be a cash merger, with Progressive
shareholders receiving up to three dlrs per share, the company
said.
    Progressive Savings has assets of about 500 mln dlrs and
operates ten offices in Los Angeles and Orange counties.
    The agreement is subject to federal and shareholder
approval.
 Reuter

['acq']

XXXXXXXXXX
New article
XXXXXXXXXX

Nippon Life Insurance Co's 538 mln dlr
purchase of a 13 pct stake in Shearson Lehman Brothers Inc
brokerage unit is a shrewd move that other Japanese insurers
are likely to follow, securities analysts said.
    The investment in one of Wall Street's top brokerage houses
is likely to pay off in dollars and international market
position, they said. "It's part of a trend towards growing
capital participation by Japanese insurance firms in foreign
financial institutions," said Simon Smithson, an analyst with
Kleinwort Benson International Inc in Tokyo.
    The investment in Shearson Lehman, a growing firm described
by some analysts as the top U.S. Retail brokerage, will give
Nippon Life a ringside seat and possibly lower commissions on
Wall Street, where it invests an increasing percentage of its
assets of 90.2 billion dlrs, they said.
    Nippon Life staff will also acquire expertise in business
sectors which have not yet opened up in Japan, they added.
    The agreement between the two companies calls for a 50-50
joint venture in London focussing on investment advisory asset
management, market research, and consulting on financing.
    Nippon Life is Japan's largest insurance company and the
world's biggest institutional investor, analysts said.
    The Japanese finance ministry is expected to approve the
deal in April, making Nippon Life the first Japanese life
insurance firm to take a stake in a U.S. Financial firm.
    The limit on foreign assets as a proportion of Japanese
insurers' assets was increased to 25 pct from 10 pct last year.
Since then, they have stepped up purchases of foreign stocks
and sought to deepen their understandng of foreign markets and
instruments.
    Last year, a Sumitomo Life Insurance Co official was
appointed to E.F. Hutton Group Inc unit E.F. Hutton and Co's
board and Sumitomo Bank Ltd spent 500 mln dlrs to become a
limited partner in Goldman, Sachs and Co.
    Smithson said Japanese banks started buying smaller and
problem-plagued banks in 1984. "But now Japanese are going for
blue-chip organisations," he said.
    "It's a reflection of what has happened in manufacturing
industries," said Brian Waterhouse at James Capel and Co. "With a
historically high yen, and historically low interest rates,
there's an increasing disincentive to invest in Japan."
    Competition in fund management has grown along with greater
Japanese savings. The typical salaried employee has 7.33 mln
yen in savings, reflecting an annual average savings rate of 17
to 18 pct, he said.
    To stay competitive, fund managers must invest overseas and
gain experience with financial instruments which are likely to
spread to Japan with further deregulation. "The high regulatory
environment has delayed (life insurance firms')
diversification. Now there's a growing number of new products
in an environment of increasing competition for performance on
fund management," Smithson said.
 REUTER

['acq']

XXXXXXXXXX
New article
XXXXXXXXXX

Former Prime Minister Robert
Muldoon, an outspoken advocate of a managed float for the N.Z.
Dollar, said the currency is at least 10 pct overvalued.
    Muldoon said in a speech last night the exchange rate
should be around 48 U.S. Cents instead of the current 57 cents.
    "A reasonable value for the New Zealand dollar would be
between 10 and 15 pct less and nearer 15 than 10. Perhaps
around about 48 cents," he said.
    The Labour Party government removed exchange controls and
floated the dollar two years ago when it was worth 44 cents.
    Muldoon has no rank in the opposition National Party, and
party leaders, with an eye to general elections to be held by
September, have rejected his calls for a managed float.
    He said the dollar was high because of "grossly excessive"
interest rates for government stock.
    "I know of no other country which is implementing such a
free floating policy," he added. "There is widespread agreement
internationally that we have no alternative to floating
currencies in the short to medium term. But we need more
effective methods of managing them so as to limit the
volatility which has caused so much concern and damage."
 REUTER

['money-fx', 'nzdlr']

-----------------------
Bucket with 25 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

THE FOLLOWING RAINFALL WAS RECORDED IN
THE AREAS OVER PAST 72 HOURS
    PARANA STATE: UMUARAMA NIL, PARANAVAI 1.5 MILLIMETRES,
LONDRINA NIL, MARINGA NIL.
    SAO PAULO STATE: PRESIDENTE PRUDENTE O.6 MM, VOTUPORANGA
12.0 MM, FRANCA 28.0 MM, CATANDUVA 10.0 MM, SAO CARLOS NIL, SAO
SIMAO NIL. REUTER11:43/VB

['coffee']

XXXXXXXXXX
New article
XXXXXXXXXX

Shr 52 cts vs 45 cts
    Net 2,623,000 vs 2,256,000
    Nine mths
    Shr 1.55 dlrs vs 1.45 dlrs
    Net 7,744,000 vs 6,542,000
 Reuter

['earn']

XXXXXXXXXX
New article
XXXXXXXXXX

Shr 13 cts vs 47 cts
    Net 13,492,000 vs 46,417,000
    Revs 720.2 mln vs 793.6 mln
    Avg shrs 99,085,000 vs 96,804,000
    NOTE: Per-share results reflect payment of preferred
dividend requirements
 Reuter

['earn']

-----------------------
Bucket with 25 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

The average price of mexico's crude
oil exports in first quarter 1987 will be 15.25 dlrs per
barrel, according to preliminary figures issued in a press
release by the state oil company Petroleos Mexicanos (PEMEX).
    It gave no direct comparison with the year-ago figure but
said crude and products sales were expected to rise to 1.99
billion dlrs this quarter, 420 mln dlrs higher than expected
and 22 pct better than the year-ago quarter.
    Prospects for the second quarter were relatively favourable
with crude exports seen at 1.320 mln bpd after an expected
1.324 mln this month, 1.323 in February and 1.395 mln in
January.
 REUTER

['crude']

XXXXXXXXXX
New article
XXXXXXXXXX

Hecla Mining Co said it
has agreed to purchase a 28 pct interest in the Greens Creek
Joint Venture from British Petroleum Co PLC's Amselco Minerals
Inc unit.
    The venture expects to bring into production a
gold-silver-lead-zinc ore body on Admiralty Island, Alaska,
containing about 3,500,000 short tons of ore assaying about
0.18 ounce of gold, 24.0 ounces of silver, 9.7 pct zinc and 3.9
pct zinc per short ton, Hecla said.  It said there is
significant potential for the discovery of additional ore.
    Hecla said initial production from a trackless underground
mine is scheduled for late 1988 at a rate of about 1,000 tons
or ore per day.  "At this rate, the Greens Creek mine will be
the largest domestic silver mine and is expected to be one of
the lowest cost producers."
    The company said it estimates its total investment in the
project, including its share of preproduction costs, at about
45 mln dlrs, to be funded through internally generated cash and
existing lines of credit.  It said Amselco will retain a
majority interest in the project.  Other interest holders are
CSX Corp <CSX> and <Exaias Resources Corp>.
 Reuter

['acq', 'gold', 'silver', 'zinc', 'lead']

XXXXXXXXXX
New article
XXXXXXXXXX

Hayes-Albion Corp said its shareholders
approved a plan to merge with and become a wholly onwed
subsidiary of privately held Harvard Industries Inc.
    St. Louis-based Harvard Industries, a manufacturer and
distributor of automobile supplies, held 80 pct of Hayes
following completion of a 13 dlrs a share cash tender offer in
December.
    Under the merger agreement, remaining shareholders of
Hayes, a Jackson, Mich.-based maker of auto supplies, will
receive 13 dlrs cash for their shares.
    Trading in Hayes common will cease at the close of business
today, the company said.
 Reuter

['acq']

\end{pythonOutput}
\textbf{Output - with permutations = 10}\\
\begin{pythonOutput}
#### Summary ####
There are 37 buckets with the following:
----------------------
Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'crude': 1, 'ship': 1})

Number of articles: 2
Topic count: Counter({'pork-belly': 1, 'acq': 1, 'livestock': 1})

Number of articles: 1
Topic count: Counter({'money-fx': 1, 'interest': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 9
Topic count: Counter({'earn': 9})

Number of articles: 1
Topic count: Counter({'money-supply': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 6
Topic count: Counter({'interest': 2, 'earn': 2, 'money-fx': 1, 'sugar': 1, 'acq': 1})

Number of articles: 1
Topic count: Counter({'orange': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 7
Topic count: Counter({'earn': 5, 'crude': 2, 'nat-gas': 1, 'copper': 1, 'acq': 1})

Number of articles: 1
Topic count: Counter({'crude': 1})

Number of articles: 1
Topic count: Counter({'acq': 1})

Number of articles: 1
Topic count: Counter({'acq': 1})

Number of articles: 2
Topic count: Counter({'interest': 1, 'acq': 1})

Number of articles: 1
Topic count: Counter({'crude': 1, 'nat-gas': 1})

Number of articles: 1
Topic count: Counter({'acq': 1})

Number of articles: 12
Topic count: Counter({'acq': 7, 'earn': 4, 'crude': 1, 'zinc': 1, 'silver': 1, 'lead': 1, 'strategic-metal': 1, 'gold': 1})

Number of articles: 1
Topic count: Counter({'interest': 1, 'gnp': 1})

Number of articles: 1
Topic count: Counter({'copper': 1, 'zinc': 1, 'silver': 1})

Number of articles: 1
Topic count: Counter({'gnp': 1})

Number of articles: 1
Topic count: Counter({'corn': 1, 'wheat': 1})

Number of articles: 31
Topic count: Counter({'earn': 14, 'grain': 4, 'acq': 4, 'money-fx': 2, 'nzdlr': 1, 'reserves': 1, 'corn': 1, 'sugar': 1, 'yen': 1, 'crude': 1, 'coffee': 1, 'cpi': 1, 'wheat': 1, 'money-supply': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'grain': 1, 'corn': 1})

Number of articles: 1
Topic count: Counter({'hog': 1, 'livestock': 1})

Number of articles: 1
Topic count: Counter({'acq': 1})

Number of articles: 2
Topic count: Counter({'acq': 2})

Number of articles: 1
Topic count: Counter({'cpi': 1})

Number of articles: 1
Topic count: Counter({'earn': 1})

Number of articles: 1
Topic count: Counter({'lei': 1})

Number of articles: 1
Topic count: Counter({'acq': 1})

Number of articles: 1
Topic count: Counter({'money-fx': 1})

#################

#### Printing 3 random articles from buckets which have more than 3 articles ####
----------------------

-----------------------
Bucket with 9 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Oper primary shr 1.03 dlr vs 2.55 dlrs
    Oper diluted shr 94 cts vs 1.76 dlrs
    Oper net 15.2 mln vs 23.4 mln
    Revs 272.0 mln vs 232 mln
    Avg shrs primary 7,625,000 vs 5,534,000
    Avg shrs diluted 12.3 mln vs 10.3 mln
    Year
    Oper shr 1.06 dlr vs 3.17 dlrs
    Oper net 39 mln vs 56.1 mln
    Revs 830.2 mln vs 657.9 mln
    Avg shrs 7,490,000 vs 5,557,000
    NOTE: 1986 oper net excludes 10.4 mln dlrs for discontinued
operations.
    1985 4th qtr excludes a loss of 4,570,000 dlrs and
6,330,000 dlrs, respectively, for discontinued operations.
    1986 oper net excludes a 10.5 mln dlr or 1.40 dlr per shr
loss from early extinquishment of notes.
    1986 and 1985 oper per share amounts are reported after
paying 31.0 mln dlrs and 38.5 mln dlrs, respectively, for
preferred stock dividends.
    1986 and 1985 4th qtr per share amounts are reported after
paying 7,292,000 dlrs and 9,333,000 dlrs, respectively, for
preferred stock dividends.
    1985's discontinued operations are restated.

 Reuter

['earn']

XXXXXXXXXX
New article
XXXXXXXXXX

Net loss 1,321,000 vs loss 1,397,000
    Sales 31.1 mln vs 29.2 mln
    1st half
    Net loss 94,000 vs loss 1,745,000
    Sales 63.0 mln vs 61.9 mln
    NOTE: Company recently went private.
    Latest quarter net includes 24,000 dlr tax credit.
    Current half net includes gain 2,041,000 dlrs pretax from
termination of pension plan.
 Reuter

['earn']

XXXXXXXXXX
New article
XXXXXXXXXX

Shr 77 cts vs 76 cts
    Net 13,843,000 vs 14,478,000
    Sales 374.6 mln vs 368.3 mln
    Avg shrs 18,003,000 vs 19,025,000
    Year
    Shr 2.16 dlrs vs 2.65 dlrs
    Net 39,503,000 vs 51,573,000
    Revs 1.41 billion vs 1.41 billion
    Avg shrs 18,269,000 vs 19,497,000
    NOTE: 1986 period ended February One
    Company changed fiscal yearend to January 31 from October
31. 1986 results were restated to reflect the change.
 Reuter

['earn']

-----------------------
Bucket with 6 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Marine Midland Banks Inc said it is
raising its prime lending rate to 7-3/4 pct from 7-1/2 pct,
effective immediately.
 Reuter

['interest']

XXXXXXXXXX
New article
XXXXXXXXXX

Champion Products Inc said its
board of directors approved a two-for-one stock split of its
common shares for shareholders of record as of April 1, 1987.
    The company also said its board voted to recommend to
shareholders at the annual meeting April 23 an increase in the
authorized capital stock from five mln to 25 mln shares.
 Reuter

['earn']

XXXXXXXXXX
New article
XXXXXXXXXX

<Industrial Equity (Pacific) Ltd>,
(IEP) the Hong Kong-listed unit of Brierley Investments Ltd
<BRYA.WE>, said it has lifted its stakes in British oil company
Ultramar Plc <UMAR.L> and U.S. Firm <Ogelbay Norton Co>.
    IEP told the Stock Exchange it now holds 36.03 mln Ultramar
shares representing 13.07 pct of the issued capital. It holds
388,600 shares in Ogelbay representing 11.68 pct of the
Delaware-based company.
    No other details were available.
 REUTER

['acq']

-----------------------
Bucket with 7 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Amoco Corp's petroleum liquids reserves
total 2.42 billion barrels at the end of 1986, down from 2.77
billion a year earlier, but its natural gas reserves increased
to 15.37 trillion cubic feet from 15.14 trillion, the company's
annual report said.
    It said the drop in crude oil and natural gas liquid
reserves reflected downward revisions of previous estimates
caused by the sharp drop in oil prices last year. This
accounted for 178 mln barrels of a worldwide downward revision
of 188 mln barrels, with 158 mln barrels of the total revision
occurring in the United States, the report said.
    Amoco said there were upward revisions in the size of its
worldwide gas reserves totalling 404 billion cubic feet last
year, while it discovered 568 billion cubic feet and purchased
298 billion cubic feet of reserves.
    Production of one trillion cubic feet offset much of these
gains, the report said.
    All of the gas reserve purchases, as well as all of the 14
mln barrels of oil reserves bought in 1986, were in the United
States, Amoco said, noting it has spent 1.1 billion dlrs to
acquire U.S. producing properties over the past three years.
    Commenting on 1987, Amoco said acquisitions "will be an
integral part of our strategy, should reserves become available
at attractive prices."
    The company said it expects "the marketing climate for
natural gas to improve in 1987, which should provide the
opportunity for Amoco to expand sales. As prices and demand
improve, we are poised to accelerate capital spending on our
inventory of attractive opportunities."
    Amoco previously announced a 1987 capital spending budget
of 3.2 billion dlrs. Such spending totaled 3.18 billion dlrs
last year, down from 5.31 billion in 1985.
 Reuter

['crude', 'nat-gas']

XXXXXXXXXX
New article
XXXXXXXXXX

Newmont Mining Corp <NEM> said Magma
Copper Co anticipates being able to produce copper at a profit
by 1991, assuming copper prices remain at their current levels.
    In an information statement distributed to Newmont
shareholders explaining the dividend of Magma shares declared
Tuesday, Newmont said Magma had a net loss of 46.6 mln dlrs in
1986, adding this was equal to 1.22 cts a share.
    Newmont holders will receive 80 pct of Magma's stock as a
dividend of one share for each of the 30,458,000 Newmont shares
now held. Newmont will retain 15 pct of the stock.
    The 1986 net loss was on a pro forma basis, Newmont said.
On a historical basis, it added, Magma had a 1986 net loss of
58.1 mln dlrs on a loss from operations of 42.3 mln dlrs.
    On Dec 31, 1986, Newmont said, Magma had about 85.0 mln
dlrs of net operating loss carryforwards expiring in 1999-2000
and about 4.0 mln dlrs of investment tax credit carryover
expiring in 2000-2001.
    Newmont said Magma has pre-tax losses of 290 mln dlrs
during the 1981 through 1985 period, noting the five major U.S.
primary copper producers reported aggregate pre-tax losses of
1.9 billion dlrs during five year period.
    Newmont said Magma had total sales of 347.3 mln dlrs last
year, including copper sales of 293.4 mln dlrs.
    It said the copper sales value was up from 267.6 mln dlrs
in 1985 reflecting a 10.1 pct increase in quantity sold to
212,000 short tons and a 0.4 pct decrease in price.
 Reuter

['earn', 'copper']

XXXXXXXXXX
New article
XXXXXXXXXX

<BP Oil Ltd>, the U.K. Marketing and
refining arm of British Petroleum Co Plc <BP.L>, raised its
pretax operating profit on a replacement cost basis to 182 mln
stg in calendar 1986, compared with 66 mln stg in 1985.
    Sales and operating revenue fell to 3.1 billion stg from
4.2 billion on a replacement cost basis. Historical cost
operating profit was 61 mln stg, up from 16 mln.
    BP Oil said 1985 profits had been depressed by exceptional
items. Its profit figures were stated before interest charges.
    Chief executive David Kendall said improved results
mirrored benefits of a restructuring program undertaken in
recent years.
    However, he warned future financial pressure on the
industry will be severe.
    "The U.K. Oil marketing and refining industry will need to
invest larger sums - probably around 500 mln stg a year - for a
good many years," he said in a statement.
 Reuter

['earn', 'crude']

-----------------------
Bucket with 12 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

ChemLawn Corp said it has
started talks on the possible sale of the company with "various
parties" that it did not identify.
    The company said the talks began after it considred and
rejected Waste Management Inc's <WMX> 27 dlr per share tender
offer for all its sharesd.
    ChemLawn gave no details on the talks.
 Reuter

['acq']

XXXXXXXXXX
New article
XXXXXXXXXX

Magnetic Technologies Corp said
it expects the second half to show continued growth in earnings
and sales.
    The company today reported a profit for the first half
ended January 31 of 105,013 dlrs, compared with a year-earlier
loss of 745,641 dlrs, on sales of 3,661,565 dlrs, up from
2,810,132 dlrs.  In all of last year, Magnetic earned 996,000
dlrs after a loss from discontinued operations of 359,000 dlrs,
on sales of 6,084,000 dlrs.
 Reuter

['earn']

XXXXXXXXXX
New article
XXXXXXXXXX

Di Giorgio Corp said it plans to
respond to an unsolicited recapitalization plan proposed by
Gabelli and Co Inc after the company, its board and its
investment bankers evaluate the proposal.
    Earlier, Gabelli said in a filing with the Securities and
Exchange Commission that it holds a 28.5 pct stake in DiGiorgio
and that it, together with Gamco Investors Inc may seek control
of the company.
    In addition, on June 25 the Gabelli group proposed to
acquire all of Di Giorgio's common shares for a combination of
20 dlrs per share in cash, a subordinated note with a face
value of eight dlrs and one common share of the post-buyout
company.
    The June 25 buyout proposal remains open until July 17.
 Reuter

['acq']

-----------------------
Bucket with 31 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

DTD Enterprises Inc said it filed an
8-K report indicating that <EaglesLair Development Corp> had
assumed control of the company under a reorganization plan
signed last month.
    The company said D. Gerald Lach, president of EaglesLair,
was named president and a director of DTD.
    In addition, DTS's board resigned and EaglesLair appointed
new directors, the company said.
 Reuter

['acq']

XXXXXXXXXX
New article
XXXXXXXXXX

Thailand's foreign reserves of gold,
special drawing rights and convertible currencies fell to 3.86
billion dlrs at end-February from 3.95 billion the previous
month, but were above the 3.08 billion held at the same time
last year, the Bank of Thailand said.
    It said the reserves were equal to about five months' worth
of imports.
 REUTER

['reserves']

XXXXXXXXXX
New article
XXXXXXXXXX

U.S. Trade Representative Clayton
Yeutter said the Export Enhancement Program, EEP, should be
used as a "tactical tool" and not as a general policy.
    Yeutter made the comment in response to a question whether
the U.S. should expand the EEP to cover grain sales to the
Soviet Union.
    He did not comment directly on the Soviet question,
replying that any decision would be made at the highest levels
of the Reagan administration, and "I don't want to preempt that."
    Yeutter told the National Grain and Feed Association EEP
should continue to be used as a tactical tool against the
European Community but not as a general policy. He said
selective EEP use has been successful in pressuring the E.C.
 Reuter

['grain']


\end{pythonOutput}
\textbf{Explanation of code and discussion}\\
We implemented the minhash algorithm from the slides of Lesson 11\footnote{\url{https://www.dropbox.com/s/8vh8nyywttf5y6e/hashing.pdf?dl=0}}.\\
We ran the algorithm on 100 random articles with 3, 5 and 10 permutations. The 100 articles was the same for each permutation with help of seed in the random library of python. For creating the bag-of-words representation we used a custom tokenizer of splitting the string and converting to lower-case.\\
~\\
\textbf{Output - permutations: 3}\\
We have one very large bucket with 91 articles in it and 7 other buckets with only one or two articles in them.\\
~\\
If we look at the large bucket we can see that it mainly consists of the topics ("earn"), accounting for ~45\%, however articles with topics related to agriculture like ("grain"), ("corn"), and perhaps ("crude") seems to have been misplaced.\\
~\\
When we look at the topics from the other buckets, we recognize topics which "should" have been in the large bucket. Although there can be some distinguish word in the text, which account for putting the article in another bucket. Or maybe its simply an error. \\
~\\

\textbf{Output - permutations: 5}\\
In this output we have 39 buckets: There are two larger buckets with 25 articles in each. ~\\
If we look at the first of them, 18 out of the 25 articles (~72\%) is in a single topic ('earn'). For the other bucket 10 out of 25 (40\%) is in a single topic ('acq'). \\
~\\
4 topics out of 9 in the second bucket, can also be found in the first bucket. One topic is in top 3 in both buckets. ~\\
This can indicate some overlap between the two buckets. We see the topics of the majority of the two buckets are related to finance. \\
~\\
\textbf{Output - permutations: 10}\\
We got 37 buckets, with the biggest bucket having 31 articles, with 14 articles (45\%) having the topic ('earn'), second-largest bucket has 12 articles with 7 containing topic ("acq") accounting for (~58\%), third-largest bucket has 9 articles with all of them containing the topic ("earn") thus (100\%).
~\\

It seems the higher number of permutations results in larger spread of buckets.
The results on the similarity of the buckets seem higher for higher permutations but are in the end inconclusive.

\end{document}
