\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage{amsmath,amssymb,graphicx}
\usepackage{cleveref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[export]{adjustbox}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal
\DeclareUnicodeCharacter{00D6}{Ö}
\DeclareUnicodeCharacter{00DC}{Ü}
\DeclareUnicodeCharacter{00E4}{ä}
\DeclareUnicodeCharacter{00E4}{ä}

% Custom colors
\usepackage{color}

\definecolor{output_background}{rgb}{0.2, 0.2, 0.2}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}
\definecolor{purple}{rgb}{0.6, 0.1, 0.9}
\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=python,
breaklines=true,
basicstyle=\ttfamily\small,
otherkeywords={1, 2, 3, 4, 5, 6, 7, 8 ,9 , 0, -, =, +, [, ], (, \), \{, \}, :, *, !},             % Add keywords here
keywordstyle=\color{blue},
emph={class, pass, in, for, while, if, is, elif, else, not, and, or, OR
    def, print, exec, break, continue, return},
emphstyle=\color{black}\bfseries,
emph={[2]True, False, None, self},
emphstyle=[2]\color{purple},
emph={[3]from, import, as},
emphstyle=[3]\color{blue},
stringstyle=\color{red},
frame=tb,
showstringspaces=false,
morecomment=[s]{"""}{"""},
commentstyle=\color{gray},
rulesepcolor=\color{blue},
title=\lstname
}}

% Python style for output highlighting
\newcommand\pythonoutputstyle{\lstset{
backgroundcolor=\color{output_background},
rulecolor=\color{output_background},
basicstyle=\ttm\small\color{white},
showstringspaces=false,
inputencoding=utf8,
extendedchars=true,
literate=%
    {á}{{\'a}}1
    {č}{{\v{c}}}1
    {ď}{{\v{d}}}1
    {é}{{\'e}}1
    {ě}{{\v{e}}}1
    {í}{{\'i}}1
    {ň}{{\v{n}}}1
    {ó}{{\'o}}1
    {ř}{{\v{r}}}1
    {š}{{\v{s}}}1
    {ť}{{\v{t}}}1
    {ú}{{\'u}}1
    {ů}{{\r{u}}}1
    {ý}{{\'y}}1
    {ž}{{\v{z}}}1
    {Á}{{\'A}}1
    {Č}{{\v{C}}}1
    {Ď}{{\v{D}}}1
    {É}{{\'E}}1
    {Ě}{{\v{E}}}1
    {Í}{{\'I}}1
    {Ň}{{\v{N}}}1
    {Ó}{{\'O}}1
    {Ř}{{\v{R}}}1
    {Š}{{\v{S}}}1
    {Ť}{{\v{T}}}1
    {Ú}{{\'U}}1
    {Ů}{{\r{U}}}1
    {Ý}{{\'Y}}1
    {Ž}{{\v{Z}}}1
    {Ö}{{\"{O}}}1
    {ö}{{\"{o}}}1
    {Ü}{{\"{U}}}1
    {ü}{{\"{u}}}1   
    {ß}{{\ss}}1
    {ä}{{\"{a}}}1
    {é}{{\'{e}}}1
    {ô}{{\^{o}}}1
    {â}{{\^{a}}}1
    {è}{{\`{e}}}1
}}


\lstnewenvironment{pythonOutput}[1][]
{
\pythonoutputstyle
\lstset{#1}
}
{}

% Python for inline
\newcommand\pythonoutput[1]{{\pythonoutputstyle\lstinline!#1!}}


% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

\title{Assignment 3\\02807 Computational Tools for Big Data}
\author{S \& A}
\date{26th October 2015}
\usepackage[cm]{fullpage}
\begin{document}

\maketitle
\newpage
%-------------- Week 5 ---------------------
\section{Exercise 8.1}
\textbf{Short recap of the exercise}\\
\textit{Define and implement a MapReduce job to count the occurrences of each word in a text file. Document that it works with a small example.}\\
~\\
\textbf{Overview}\\
We implement the MapReduce jobs by creating mappers and reducers. A mapper takes key/value pairs and generates key/value pairs, a reducer iterates through values for each distinct key, and generates an output.\\\\
For this exercise, the mapper takes a line as value, then splits them and yields each word and the count 1. The reducer sums the value for each unique key and yields the result. 
\\*
~\\
\textbf{Code}
\pythonexternal{"../Lesson 8/exercise-8-1.py"}
~\\
\textbf{Input}
\begin{pythonOutput}
der var engang en mand
der boede i en spand
spanden var af ler
nu kan jeg ikke mer
\end{pythonOutput}
\textbf{Output}
\begin{pythonOutput}
"af"    1
"boede"    1
"der"   2
"en"    2
"engang"  1
"i" 1
"ikke"  1
"jeg"   1
"kan"   1
"ler"   1
"mand"  1
"mer"   1
"nu"    1
"spand" 1
"spanden"   1
"var"   2
\end{pythonOutput}
\textbf{Explanation of output and discussion}\\
We can see it returns the key (word) and the sum of the ones, thus the word count.~\\

\section{Exercise 8.2}
\textbf{Short recap of the exercise}\\
\textit{Define and implement a MapReduce job that determines if a graph has an Euler tour (all vertices have even degree) where you can assume that the graph you get is connected.
It is fine if you split the file into five different files. You do not need to keep the node and edge counts in the top of the file.}\\
~\\
\textbf{Overview}\\
For this exercise we use one mapper and two reducers.~\\
The steps method tells MrJob in which order the methods should run. The order of execution is mapper, reducer, reducer2.

\begin{itemize}
  \item \textit{mapper} - takes each line and split it into from- and to-edges. Then it yields these two as (key,1) pairs
  \item \textit{reducer} - sums the values of the key, as we did in exercise 8.1
  \item \textit{reducer2} - checks if all values of the key is even\\
\end{itemize}
~\\
We  split the input file in five files and get a concatenation of outputs with the following bash command: ~\\
ls graph*.txt | xargs -n1 ./exercise-2.py -q \\
~\\
This runs the script with all graph files as arguments in ascending order.

~\\
\textbf{Code}
\pythonexternal{"../Lesson 8/exercise-2.py"}
~\\
\textbf{Output}
\begin{pythonOutput}
"Has Euler Tour"    true
"Has Euler Tour"    false
"Has Euler Tour"    true
"Has Euler Tour"    true
"Has Euler Tour"    false

\end{pythonOutput}


\section{Exercise 8.3}
\textbf{Short recap of the exercise}\\
\textit{Implement the MapReduce job from the lecture which finds common friends (note that for the Facebook file, you need to extend the job to convert from a list of edges to the format from the slides – do this with an additional map/reduce job).}\\
~\\
\textbf{Overview}\\
We split this assignment in two: one for the example in the slides, and one for the facebook file. ~\\
\textbf{For the slides example}\\
For finding common friends of the slides we use a \textit{mapper} and \textit{reducer} drawing inspiration from the Lecture 8 slides\footnote{\url{https://www.dropbox.com/s/xogs1ozzq7aqbod/mapreduce.pdf?dl=0}}:
\begin{itemize}
\item \textit{mapper} - converts to sorted node pairs and their common friends, then we create two groups based on node pair key.
\item \textit{reducer} - reduce by yielding the intersection of the two groups.
\end{itemize}
The functionality of the \textit{main\_mapper} and \textit{main\_reducer} are similar to the previous exercise.
~\\
\textbf{For the facebook file}
For finding common friends of the Facebook file we convert from a list of edges to the slides format. This is done by the \textit{convert\_mapper} and \textit{convert\_reducer}:
\begin{itemize}
\item \textit{convert\_mapper} - simply duplicates the to- and -from edge in (to,from) and (from, to) pairs.
\item \textit{convert\_reducer} - reduce by key, creating a tuple of neighbor nodes.
\end{itemize}
The functionality of the \textit{main\_mapper} and \textit{main\_reducer} are similar to the previous exercise.
~\\
\textbf{Code - Slides example}
\pythonexternal{"../Lesson 8/exercise-3-1.py"}
\textbf{Output - Slides example}
\begin{pythonOutput}
["A", "B"]  ["C", "D"]
["A", "C"]  ["B", "D"]
["A", "D"]  ["B", "C"]
["B", "C"]  ["A", "D", "E"]
["B", "D"]  ["A", "C", "E"]
["B", "E"]  ["C", "D"]
["C", "D"]  ["A", "B", "E"]
["C", "E"]  ["B", "D"]
["D", "E"]  ["B", "C"]


\end{pythonOutput}
\textbf{Code - Facebook file}
\pythonexternal{"../Lesson 8/exercise-3-2.py"}
\textbf{Output - Facebook file}
\begin{pythonOutput}
[0, 100]    [119, 150, 163, 189, 217, 269, 323, 64]
[0, 101]    [180, 187, 194, 204, 24, 242, 249, 254, 266, 299, 302, 317, 330, 346, 53, 80, 92, 
             94]
[0, 102]    [175, 227, 263, 296, 99]
[0, 103]    [136, 169, 172, 185, 200, 211, 25, 252, 271, 285, 323, 339, 56, 7, 98]
[0, 104]    [109, 113, 122, 123, 128, 142, 169, 186, 188, 200, 203, 21, 212, 239, 25, 252, 26, 
             271, 277, 295, 303, 318, 322, 325, 332, 344, 45, 55, 56, 67, 98]
[0, 105]    [119, 148, 21, 236, 25, 257, 272, 277, 280, 315, 39, 69, 9]
[0, 106]    [169, 231, 238, 29, 329, 332, 88]
[0, 107]    [171, 58]
[0, 108]    [127, 159, 184, 197, 21, 251, 272, 281, 284, 320, 36, 57]
[0, 109]    [104, 118, 119, 122, 13, 142, 148, 158, 169, 186, 200, 203, 21, 229, 239, 252, 26, 
             271, 277, 285, 295, 297, 303, 304, 31, 314, 322, 323, 324, 325, 331, 332, 50, 56, 
             67, 98]


\end{pythonOutput}


\section{Exercise 8.4}
\textbf{Short recap of the exercise}\\
\textit{Make a MapReduce job which counts the number of triangles in a graph.}\\
~\\
\textbf{Overview}\\
The convert\_mapper, convert\_reducer and main\_mapper steps are the same as in exercise 8.3. ~\\
The other steps are:\\
 ~\\
 \begin{itemize}
\item \textit{main\_reducer} -  finds common "friends" (nodes) of its key-values. The new key is the original key and the new friend ( [old\_key, friend] ). It yields the new key and the value 1.
\item \textit{sum\_reducer} - to reduce by key and sums the value.
\item \textit{count\_triangle\_mapper} - checks if both length of key is 3 and value is 3. If it is, it yield 1 as value, else 0. The key is the summation of what we wil find: "Number of Triangles"
\item \textit{sum\_triangle\_reducer} - to reduce by key and sum the value, which gives us the final result.
\end{itemize}
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 8/exercise-8-4.py"}
~\\
\textbf{Output}
\begin{pythonOutput}
./exercise-8-4-v2.py 8.4_small_test.txt -q
"Number of Triangles"   120676
\end{pythonOutput}
\textbf{Explanation of output and discussion}\\
We can see we found 120676 triangles in the graph of the road network of California.~\\
From their website\footnote{\url{http://www.cise.ufl.edu/research/sparse/matrices/SNAP/roadNet-CA.html}} we can see their number of triangles found corresponds with ours. 


%-------------- Week 9 ---------------------
\section{Exercise 9.1}
\textbf{Short recap of the exercise}\\
\textit{Write a Spark job to count the occurrences of each word in a text file. Document that it works with a small example}\\
~\\
\textbf{Overview}
~\\
Following is a summarization of the comments in the file. 
\begin{enumerate}
  \item Extract lines in the file
  \item Splitting the lines into words
  \item Creating a pair for each word in the form ( word , count ) where count is the occurrence of the word , set to 1
  \item Counting the words
  \item Get the result and sort it based on count
  \item Print out the result\\
\end{enumerate}
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 9/exercise-9-1.py"}~\\
\textbf{Output}~\\
\begin{pythonOutput}
'en' has an occurrences of 3
'der' has an occurrences of 2
'var' has an occurrences of 2
'spand' has an occurrences of 1
'spanden' has an occurrences of 1
'af' has an occurrences of 1
'i' has an occurrences of 1
'kan' has an occurrences of 1
'booede' has an occurrences of 1
'ikke' has an occurrences of 1
'ler' has an occurrences of 1
'mand' has an occurrences of 1
'gang' has an occurrences of 1
'mer' has an occurrences of 1
'nu' has an occurrences of 1
'jeg' has an occurrences of 1
\end{pythonOutput}
~\\
\textbf{Explanation of output and discussion}\\
We can see that the output is identical with exercise 8.1, and since we have validated the result i 8.1, this result is also correct. 

\section{Exercise 9.2}
\textbf{Short recap of the exercise}\\
\textit{Write a Spark job that determines if a graph has an Euler tour (all vertices have even degree) where you can assume that the graph you get is connected.~\\
It is fine if you split the file into 5 different files. You do not need to keep the node and edge counts in the top of the file.}\\
~\\
\textbf{Overview}
As in the exercise in Week 8, we have splittet the file into 5 different files. ~\\
We have created a python function 'has\_euler\_tour' which take a filename string as argument. \\
~\\
The following is a step-by-step explaination of the code (primarily the function). 
\begin{enumerate}
  \item Loading in the file
  \item Creating a pair of integer values in the form (from\_node , to\_node), by splitting on whitespace
  \item Creating a list of pairs in the form [(from\_node , to\_node), (to\_node , from\_node)]
  \item Creating a pair for each node in position 1 of the pairs in the list. The new pair has the form (node, count) where count is the occurrence of the node, set to 1
  \item We use reduceByKey to summerize the values. reduceByKey returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function a+b. 
  \item filter-function find those whos values are not even
  \item If the count of result from filter-function is 0, return true, else return false 
  \item Print out the result\\
\end{enumerate}
What the code actually does, is to count the edges for each node. When we create the list of opposite pairs (step 3), we account for both to and from edges, which mean we can simply count the occurences of the node in positon 1 (step 4). ~\\
From this we can decide whether it has Euler tour. 
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 9/exercise-9-2.py"}~\\
\textbf{Output}~\\
\begin{pythonOutput}
Has Euler Tour: True
Has Euler Tour: False
Has Euler Tour: True
Has Euler Tour: True
Has Euler Tour: False
\end{pythonOutput}
~\\
\textbf{Explanation of output and discussion}\\
We can see that the output is identical with exercise 8.2, so we assume this result is correct. 

 ~\\

\section{Exercise 9.3}
\textbf{Short recap of the exercise}\\

\textit{Compute the following things using Spark:
\begin{enumerate}
    \item What are the 10 networks I observed the most, and how many times were they observed? Note: the bssid is unique for every network, the name (ssid) of the network is not necessarily unique.
    \item What are the 10 most common wifi names? (ssid)
    \item What are the 10 longest wifi names? (again, ssid)
\end{enumerate}}
\textbf{Overview}~\\
We have created a single file with the code for all three tasks, for each task we will explain the relevant functions used if they differ from the previous task. ~\\

\textbf{What are the 10 networks I observed the most, and how many times were they observed?}~\\
We load the wifi.data file, then we extract bssid from it by converting it to a dict usin the eval() function. We make a pair (bssid, count) where count is 1. Then we add the counts together with reduceByKey and print out top 10 in descending order (by count). ~\\

 \textbf{What are the 10 most common wifi names? (SSID)}~\\
We extract distinctive pairs of SSIDs and BSSID using the distinct function, but omit duplicate access points.
We then count the SSIDs and print them like in the previous exercise. ~\\

 \textbf{What are the 10 longest wifi names? (SSID)}~\\
We get the distinct SSID, Then we print out top 10 in descending order by using the negative length of the SSID as the key. ~\\~\\ 
\textbf{Code}
\pythonexternal{"../Lesson 9/exercise-9-3.py"}~\\
\textbf{Output - What are the 10 networks I observed the most, and how many times were they observed? Note: the BSSID is unique for every network, the name (SSID) of the network is not necessarily unique.}
\begin{pythonOutput}
[(u'34:21:09:12:6c:1a', 347),
 (u'00:24:b2:98:39:d2', 338),
 (u'34:21:09:12:6c:18', 324),
 (u'e8:08:8b:c9:c1:79', 318),
 (u'44:94:fc:56:08:fb', 315),
 (u'00:22:b0:b3:f2:ea', 314),
 (u'2c:b0:5d:ef:08:2b', 272),
 (u'44:94:fc:56:ce:5e', 240),
 (u'28:cf:e9:84:a1:c3', 211),
 (u'bc:ee:7b:55:1a:43', 210)]
\end{pythonOutput}
\textbf{Output - What are the 10 most common wifi names? (SSID)}
\begin{pythonOutput}
[(u'', 15),
 (u'SIT-GUEST', 15),
 (u'SIT-BYOD', 13),
 (u'SIT-PROD', 13),
 (u'PDA-105', 11),
 (u'KNS-105', 11),
 (u'MED-105', 11),
 (u'KEA-PUBLIC', 6),
 (u'GST-105', 5),
 (u'wireless', 4)]
\end{pythonOutput}
\textbf{Output - What are the 10 longest wifi names? (SSID)}
\begin{pythonOutput}
[u'HP-Print-43-Deskjet 3520 series',
 u'TeliaGatewayA4-B1-E9-2C-9E-CA',
 u'TeliaGateway08-76-FF-84-FF-8C',
 u'TeliaGateway9C-97-26-57-15-F9',
 u'TeliaGateway08-76-FF-46-3E-36',
 u'TeliaGateway9C-97-26-57-15-99',
 u'TeliaGateway08-76-FF-8A-EE-32',
 u'TeliaGateway08-76-FF-85-04-2F',
 u'TeliaGateway08-76-FF-9C-E0-82',
 u'Charlotte R.s Wi-Fi-netv\xe6rk']

\end{pythonOutput}
\textbf{Explanation of code and discussion}
From the outputs we can see that the network with BSSID 34:21:09:12:6c:1a was observed the most, the most common wifi name was empty string and SIT-GUEST and the longest wifi name was HP-Print-43-Deskjet 3520 series.


%-------------- Week 10 ---------------------
\section{Exercise 10.1}
\textbf{Short recap of the exercise}\\
\textit{Explain in your own words what Deep Learning is.}~\\
\textbf{Answer}\\
Deep Learning (DL) is a subset/branch of Machine Learning (which is the study of algorithms which can improve themselves over time).\\

DL is used as any other Machine Learning technique, to learn from data, and predict an outcome based on the trained model. DL is based on Artificial Neural Networks (ANN - a Machine Learning model), an ANN is a machine learning model inspired by the neural network of the brain, it consists of a large number of interconnected weighted neurons which can activate each other and the weights can be tuned which makes it adaptable and able to learn. The difference between ANN and DL is, that DL trains multi-layered or hierarchical ANN's.\\

A DL model is able to use a technique called representational learning to learn features of the dataset, thus when you give it a raw representation of the data (that is human interpretable), it learns itself what the different features are and it's then able to do recognition/supervised learning.\\

The ANN's each learns a feature through a nonlinear transformation and pass it along to the next ANN in the hierarchy until it gets to a classifier. Through each ANN the features learned are higher representations of the data. For example for facial recognition, for the first ANN receiving the input data, the feature learned is edges, then the next ANN learns different combinations of edges forming a part of a face, and so on.\\

Deep learning is very effective dealing with unstructured data like media (sounds, images, videos, text, time-series data)\\

It can be used for recommender systems and thus try to predict a rating or peference an user would give to an item, this is used currently for media streaming services like Spotify or Netflix, which can recommend new music or shows based on your playlist.\\

\section{Exercise 10.2}
\textbf{Short recap of the exercise}\\
\textit{Explain in your own words what a Convolutional Neural Network is.}\\
~\\
\textbf{Answer}\\
A Convolutional Neural Network (CNN) is a variant of the Artificial Neural Network (ANN). It is inspired by the human visual system and is typically used in image recognition.\\

It works similarly to a (ANN) by using layers of neurons where each layer feeds into the next layer and for each layer, the layer will obtain higher-level features.\\

However where in an ANN each neuron in a given layer feeds into all the neurons in the next layer, in a CNN a subset or "patch" of neurons feeds into a number of neurons in the next layer, that patch of neurons is called the local receptive field, for image recognition, this means small "overlapping windows" of the input image is building the next layer resulting in a slightly smaller layer than the former. In a CNN each layer will have the same weights and bias and thus detect the same feature just at different windows in the image. Using the same weights and bias reduce the parameters involved and computational power needed.\footnote{\url{http://neuralnetworksanddeeplearning.com/chap6.html}}\\

A CNN often makes use of a max-pooling layer, in between convolutional layers, which takes a maximum of the features over segments of the input layer, to reduce the spatial size of the data thus reducing the amount of parameters, and makes the model invariant to very small transformations of the data (overfitting).\footnote{\url{http://colah.github.io/posts/2014-07-Conv-Nets-Modular/}}.\\

With neural network (and other commonly machine learning techniques), feature engineering is one of the most essential and important task for good classification\footnote{\url{http://blog.kaggle.com/2014/08/01/learning-from-the-best/}}. Feature engineering is a very hard and complex task, which you would have to do for each type of problems. Using CNN, feature engineering is beeing done when we train it. 
For each training, it gets better and better to filter the input for relevant information (feature engineering). Once we learned our hierarchical features, we can simply pass them to a fully connected, simple neural network that combines them in order to classify the input into classes\footnote{\url{http://timdettmers.com/2015/03/26/convolution-deep-learning/}}.\\

\section{Exercise 10.3}
\textbf{Short recap of the exercise}\\
\textit{In this exercise you will be using the Caffe Deep Learning framework to classify a picture of a cat, and a picture of your own choosing.}\\
~\\
\textbf{Overview}\\
We started the virtual machine by running \textit{vagrant up} and navigating via our browser to \textit{localhost:8003}\\
We executed the code contained in the Jupyter notebook with a series of pictures: the default cat picture, a picture of a pig as seen in Figure \ref{fig:pig}, a picture of a beer glass \ref{fig:beer_glass}, a picture of a cloud and below each picture we got an output.
~\\
\textbf{Output - cat}\\
\begin{pythonOutput}
Predicted class is #282.
['n02123159 tiger cat' 'n02123045 tabby, tabby cat'
 'n02124075 Egyptian cat' 'n02119022 red fox, Vulpes vulpes'
 'n02127052 lynx, catamount']
\end{pythonOutput}
\textbf{Output - pig}
\begin{figure}[h!]
\begin{center}
\caption{Picture of a pig.}
\label{fig:pig}
\includegraphics[scale=0.5]{"../Lesson 10/images/pig"}
\end{center}
\end{figure}
\begin{pythonOutput}
Predicted class is #341.
['n02395406 hog, pig, grunter, squealer, Sus scrofa'
 'n01632777 axolotl, mud puppy, Ambystoma mexicanum'
 'n03223299 doormat, welcome mat' 'n02328150 Angora, Angora rabbit'
 'n02412080 ram, tup']
\end{pythonOutput}
\textbf{Output - beer glass}
\begin{figure}[h!]
\begin{center}
\caption{Picture of a beer glass.}
\label{fig:beer_glass}
\includegraphics[scale=0.25]{"../Lesson 10/images/beer_glass"}
\end{center}
\end{figure}
\begin{pythonOutput}
Predicted class is #855.
['n04423845 thimble' 'n02823750 beer glass'
 'n07615774 ice lolly, lolly, lollipop, popsicle'
 'n04131690 saltshaker, salt shaker' 'n02948072 candle, taper, wax light']
\end{pythonOutput}

\textbf{Output - cloud}
\begin{figure}[h!]
\begin{center}
\caption{Picture of cloud.}
\label{fig:cloud}
\includegraphics[scale=0.5]{"../Lesson 10/images/cloud"}
\end{center}
\end{figure}
\begin{pythonOutput}
Predicted class is #190.
['n02095889 Sealyham terrier, Sealyham' 'n04266014 space shuttle'
 'n09288635 geyser' 'n02948072 candle, taper, wax light' 'n03825788 nipple']
\end{pythonOutput}
\textbf{Explanation of code and discussion}\\

The predicted class for the default cat picture results in a correct classification as "tabby" or "tabby cat" among the top 4 answers, we then tried the pig picture in Figure \ref{fig:pig} which we can read resulted in the correct classification as hog/pig. Next we tried somewhat to trick it by showing it an alternatively shaped beer glass as shown in Figure \ref{fig:beer_glass}, this was also correctly identified in the second answer, next we tried a cloud as seen in Figure \ref{fig:cloud}, and it seemed to do the trick as none of the answers given by the model were correct.
%-------------- Week 11 ---------------------
\section{Exercise 11.1}
\textbf{Short recap of the exercise}\\
\textit{Train a random forest classifier to predict the topic earn in the articles, then implement it using feature hashing with 1000 buckets}\\
~\\
\textbf{Overview}
\textbf{Code}\\
\pythonexternal{"../Lesson 11/exercise_11_1.py"}
~\\
\textbf{Output}
\begin{pythonOutput}
Accuracy using a bag-of-words representation: 
(10377, 70793)
0.958092485549
Accuracy using feature-hashing with 2 buckets:
(10377, 2)
0.721579961464
Accuracy using feature-hashing with 10 buckets:
(10377, 10)
0.874759152216
Accuracy using feature-hashing with 100 buckets:
(10377, 100)
0.926782273603
Accuracy using feature-hashing with 500 buckets:
(10377, 500)
0.933526011561
Accuracy using feature-hashing with 1000 buckets:
(10377, 1000)
0.939306358382


\end{pythonOutput}
\textbf{Explanation of code and discussion}\\
We implemented feature-hashing using the Exercise description and the Lesson 11 slides\footnote{\url{https://www.dropbox.com/s/8vh8nyywttf5y6e/hashing.pdf?dl=0}}.\\
We created the bag-of-words representation with a custom tokenizer that split the strings and lowercased the tokens.\\
In the articles dataset we found that 3776 articles contained the topic earn out of the total of 10377 articles, by simply guessing this would mean an accuracy of 63.611\%. In the output we can see the shape and accuracies of feature-hashing with a number of buckets. We find that even with only 2 buckets, the accuracy is 72.157\%, and for a bigger number of buckets the accuracy is nearing the accuracy of the bag-of-words representation. For the bag-of-words representation with 70793 features the accuracy is 95.809\% and for feature-hashing with 1000 buckets it is 93.930\%.

\section{Exercise 11.2}
\textbf{Short recap of the exercise}\\
\textit{Implement your own MinHash algorithm. Try with different number of hash functions/permutations (for example 3, 5, 10).
Look at which documents end up in the same buckets. Do they look similar? Do they share the same topics?}\\
~\\
\textbf{Overview}\\
See our description after the sourcecode and outputs\\
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 11/exercise_11_2.py"}
~\\
\textbf{Output - with permutations = 3}\\
\begin{pythonOutput}
#### Summary ####
There are 1 buckets with the following number of articles in them:
----------------------
100
#################

#### Printing 3 random articles from buckets which have more than 3 articles ####
----------------------

-----------------------
Bucket with 100 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Carolyn Bean Publishing Ltd said
it has completed the acquisition of greeting card company
Millen Cards, which had sales of over 1,500,000 dlrs in 1986,
for undisclosed terms.
    The company said 90 pct of Millen's cards have been sold in
the northeast and mid-Atlantic states, but Bean expects to
increase sales to 2,500,000 dlrs this year by distributing the
line nationally.
    Millen specializes in Jewish religious cards and also sells
traditional greeting cards.
 Reuter

['acq']

XXXXXXXXXX
New article
XXXXXXXXXX

Pantera's Corp said it agreed to
buy ten pizza restaurants in southeastern Colorado from
creditors foreclosing on the facilities.
    The purchase price includes 1.25 mln dlrs in cash and
company stock, it said.
    Separately, Pantera's said it issued an area development
agreement with a franchisee group for northeastern Colorado,
including the Denver area, for the opening of about 20
franchised Pantera's pizza restaurants.
 Reuter

['acq']

XXXXXXXXXX
New article
XXXXXXXXXX

Schlumberger Ltd said it terminated an
agreement in principle for Fujitsu Ltd to buy 80 pct of its
Fairchild Semiconductor operations.
    The company said the rising political controversy in the
U.S. concerning the venture made it unlikely that the sale of
the Fairchild stake could be completed within a reasonable
time.
    The sale has been opposed by the U.S. Commerce Department
and the U.S. Defense Department, in part on national security
grounds.
    The company said termination of the agreement opened other
possibilities, including a possible leveraged buyout of the
semiconductor maker by Fairchild management.
    In the interim, Fairchild would continue its ongoing
business within Schlumberger, the oilfield services concern
said.
    Last October, Schlumberger announced the sale of the
Fairchild stake and said it would take a 200 mln dlrs charge in
the fourth quarter from the sale. The company ended up
recording special charges of 2.1 billion dlrs in the fourth
quarter, leading to a loss of 2.02 billion dlrs for the year.
    Schlumberger never announced a price for the sale, but
industry analysts have estimated the value of the deal at about
200 mln dlrs.
    The proposed sale was under antitrust review by the U.S.
Justice Department. Additionally, Commerce Secretary Malcolm
Baldridge and other U.S. officials have voiced reservations
about the transaction since it was announced.
    Government officials have expressed concern that the sale
could reduce the competitiveness of U.S. chip makers by putting
key advanced technology into Japanese hands.
    New, high-technology semiconductors are used in
supercomputers, which are faster and more powerful than
existing computers.
    Schlumberger is an oilfield services company controlled by
French interests and headquartered in New York. Fujitsu Ltd is
a computer and telecommunications company based in Japan.
 Reuter

['acq']

\end{pythonOutput}
\textbf{Output - with permutations = 5}\\
\begin{pythonOutput}
#### Summary ####
There are 6 buckets with the following number of articles in them:
----------------------
1
4
1
1
1
92
#################

#### Printing 3 random articles from buckets which have more than 3 articles ####
----------------------

-----------------------
Bucket with 4 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Bundesbank vice president Helmut
Schlesinger said he saw no reason to lower interest rates now.
    With money supply growth showing no sign of slowing down in
May and the dollar stable or even rising against the mark,
Schlesinger told Reuters that he was not convinced that a
further cut in interest rates was needed.
    The economy is picking up, after contracting by a
seasonally adjusted 1/2 to one pct in the first quarter from
the fourth, he added. "We may have an increase in gnp starting
in the second quarter," he said in an interview.
    Concerned by the first quarter downturn, the U.S. Has been
pressing West Germany to pump up its economy and boost its
imports, either through fiscal or monetary policy.
    Schlesinger said the contraction in the first quarter was
mainly due to adverse weather conditions, just as occured in
1986. Year-on-year growth was thus about two pct.
    He estimated that economic growth for the year as a whole
would probably be between one and two pct.
   "It is not a question of monetary conditions if domestic
demand does not grow strongly," he said, noting that interest
rates are at historically low levels and funds are ample.
    Schlesinger said he saw no signs that central bank money
stock growth was slowing down from its recent year-on-year pace
of 7-1/2 to eight pct, well above the Bundesbank's three to six
pct target.
    He said the target could still be achieved but much will
depend on the direction of long-term capital flows. Heavy
inflows, particularly in January around the time of the EMS
revaluation, boosted domestic money supply.
    "There is still a certain hope that the net inflow of
foreign money can be diminished or can even be a little bit
reversed," Schlesinger said.
    A major reason for the inflows was the market's conviction
that the mark was headed higher. "As we can see from the market,
expectations for a further revaluation of the deutschemark have
diminished," Schlesinger said.
    The recent widening of interest rate differentials, the
fact that the dollar has fallen sharply in a very short period
and an improvement in real trade balances have all combined
towards stabilizing the dollar, he said.
    Asked if central banks might act to prevent a sharp dollar
rise, as the U.S. Did in March when the dollar rose above 1.87
marks, he said this would depend on the circumstances.
    At midday here, the dollar stood at 1.8340/45 marks.
    "Central banks are always in contact about these
fluctuations but I cannot give any answer how they would react,"
Schelsinger said.
    "One has to look at how it (the market) is moving," he said,
adding, "It is not only our own case, it is also the American
case."
    He said that the West German export industry has been hit
hard by the dollar's sharp fall and would probably like to see
some correction now. "But it wouldn't be good to have short-term
fluctuations," he said. "Let us wait and see."
    "It is mainly the strength of the (dollar) fall in a very
short period which was a little bit of a shock, than the level
(of rates) as such," Schlesinger said.
    The sharp rise of the mark, coupled with weak prices of
such key commodities as oil, had a favourable impact on West
German inflation down year.
    Although there have recently been signs of inflation
picking up, he said that this was due to changes in key
commodity prices. The underlying inflation rate this year would
be unchanged, at about one to 1-1/2 pct, he said.
    Schlesinger said the problem of rapid money supply growth
was longer term, in that the economy was building up the
potential for a possible eventual resurgence of inflation.
    The above-target growth of money supply over the past 16
months had prompted some discussion of the usefulness of
targets themselves, a matter which might be taken up at the
mid-year meeting of the Bundesbank's council, Schlesigner said.
    But he added: "I don't see any great pressure to go away
from it."
 REUTER

['interest', 'gnp']

XXXXXXXXXX
New article
XXXXXXXXXX

The cost of living in North
Rhine-Westphalia, Germany's most populous state, fell 0.1 pct
in the month to mid-March to stand 0.5 pct lower than at the
same time a year earlier, the regional statistics office said.
    Prices had risen 0.3 pct in the month to mid-February but
had fallen 0.7 pct year-on-year.
    The regional figures are considered a good guide to
national inflation trends. The Federal Statistics Office is due
to publish provisional national figures for March by the end of
this month.
 REUTER

['cpi']

XXXXXXXXXX
New article
XXXXXXXXXX

Thailand's foreign reserves of gold,
special drawing rights and convertible currencies fell to 3.86
billion dlrs at end-February from 3.95 billion the previous
month, but were above the 3.08 billion held at the same time
last year, the Bank of Thailand said.
    It said the reserves were equal to about five months' worth
of imports.
 REUTER

['reserves']

-----------------------
Bucket with 92 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Shr 63 cts vs 50 cts
    Net 73.8 mln vs 62.3 mln
    Sales 664.2 mln vs 600.6 mln
    Nine mths
    Shr 2.06 dlrs vs 1.67 dlrs
    Net 241.2 mln vs 205.9 mln
    Sales 2.04 billion vs 1.80 billion
    Avg shrs 117.1 mln vs 123.4 mln
 Reuter

['earn']

XXXXXXXXXX
New article
XXXXXXXXXX

<National Beverage Corp>
said it agreed to acquire privately held Faygo Beverages Inc
for an undisclosed amount of cash.
    The company said Detroit-based Faygo, a soft drink maker,
has annual revenues of more than 100 mln dlrs.
    National Beverage, which is also privately held, owns and
bottles Shasta Beverages, Spree All Natural Beverages and
private label brands in its 11 bottling facilities in the
United States.
 Reuter

['acq']

XXXXXXXXXX
New article
XXXXXXXXXX

Champion Products Inc said its
board of directors approved a two-for-one stock split of its
common shares for shareholders of record as of April 1, 1987.
    The company also said its board voted to recommend to
shareholders at the annual meeting April 23 an increase in the
authorized capital stock from five mln to 25 mln shares.
 Reuter

['earn']

\end{pythonOutput}
\textbf{Output - with permutations = 10}\\
\begin{pythonOutput}
#### Summary ####
There are 15 buckets with the following number of articles in them:
----------------------
69
1
1
1
1
1
3
13
2
1
2
1
2
1
1
#################

#### Printing 3 random articles from buckets which have more than 3 articles ####
----------------------

-----------------------
Bucket with 69 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Shr 63 cts vs 50 cts
    Net 73.8 mln vs 62.3 mln
    Sales 664.2 mln vs 600.6 mln
    Nine mths
    Shr 2.06 dlrs vs 1.67 dlrs
    Net 241.2 mln vs 205.9 mln
    Sales 2.04 billion vs 1.80 billion
    Avg shrs 117.1 mln vs 123.4 mln
 Reuter

['earn']

XXXXXXXXXX
New article
XXXXXXXXXX

Newmont Mining Corp <NEM> said Magma
Copper Co anticipates being able to produce copper at a profit
by 1991, assuming copper prices remain at their current levels.
    In an information statement distributed to Newmont
shareholders explaining the dividend of Magma shares declared
Tuesday, Newmont said Magma had a net loss of 46.6 mln dlrs in
1986, adding this was equal to 1.22 cts a share.
    Newmont holders will receive 80 pct of Magma's stock as a
dividend of one share for each of the 30,458,000 Newmont shares
now held. Newmont will retain 15 pct of the stock.
    The 1986 net loss was on a pro forma basis, Newmont said.
On a historical basis, it added, Magma had a 1986 net loss of
58.1 mln dlrs on a loss from operations of 42.3 mln dlrs.
    On Dec 31, 1986, Newmont said, Magma had about 85.0 mln
dlrs of net operating loss carryforwards expiring in 1999-2000
and about 4.0 mln dlrs of investment tax credit carryover
expiring in 2000-2001.
    Newmont said Magma has pre-tax losses of 290 mln dlrs
during the 1981 through 1985 period, noting the five major U.S.
primary copper producers reported aggregate pre-tax losses of
1.9 billion dlrs during five year period.
    Newmont said Magma had total sales of 347.3 mln dlrs last
year, including copper sales of 293.4 mln dlrs.
    It said the copper sales value was up from 267.6 mln dlrs
in 1985 reflecting a 10.1 pct increase in quantity sold to
212,000 short tons and a 0.4 pct decrease in price.
 Reuter

['earn', 'copper']

XXXXXXXXXX
New article
XXXXXXXXXX

Esso Malaysia Bhd, a unit of Exxon
Corp of the U.S., Reported net profit of 70 mln ringgit from
its petroleum and ammonia operations in 1986 compared with 48.7
mln in 1985. Chairman Gerald F Cox said the improved
performance was mainly due to product prices falling more
slowly than crude prices during the year.
    He added that total sales volume increased as a result of
higher offtake by affiliated companies, while inland market
sales were maintained at around the previous year's levels.
    But growth prospects in 1987 remained weak and 1986 results
are unlikely to be repeated in the current financial year.
 REUTER

['earn']

-----------------------
Bucket with 13 articles in it
-----------------------


XXXXXXXXXX
New article
XXXXXXXXXX

Treasury balances at the Federal
Reserve fell on April 10 to 3.373 billion dlrs from 3.523
billion dlrs on the previous business day, the Treasury said in
its latest budget statement.
    Balances in tax and loan note accounts fell to 11.645
billion dlrs from 11.869 billion dlrs on the same respective
days.
    The Treasury's operating cash balance totaled 15.018
billion dlrs compared with 15.392 billion dlrs on April 9.
 Reuter

['money-supply']

XXXXXXXXXX
New article
XXXXXXXXXX

Champion Products Inc said its
board of directors approved a two-for-one stock split of its
common shares for shareholders of record as of April 1, 1987.
    The company also said its board voted to recommend to
shareholders at the annual meeting April 23 an increase in the
authorized capital stock from five mln to 25 mln shares.
 Reuter

['earn']

XXXXXXXXXX
New article
XXXXXXXXXX

Qtr ends April 30
    Shr loss 10 cts vs profit nine cts
    Net loss 163,465 vs profit 131,815
    Revs 3,672,731 vs 3,763,829
    Nine mths
    Shr profit four cts vs profit one ct
    Net profit 57,911 vs profit 11,380
    Revs 11,753,950 vs 10,794,822
 Reuter

['earn']

\end{pythonOutput}
\textbf{Explanation of code and discussion}\\
We implemented the minhash algorithm from the slides of Lesson 11\footnote{\url{https://www.dropbox.com/s/8vh8nyywttf5y6e/hashing.pdf?dl=0}}.\\
We ran the algorithm on 100 random articles with 3, 5 and 10 permutations. For creating the bag-of-words representation we used a custom tokenizer of splitting the string and converting to lower-case.\\\\
\textbf{Output - permutations: 3}\\
We can see that all of the 100 articles has been put in the same bucket, that is they are categorized as similar. If we look at the three (random chosen) articles and compare them, all of them seem to be related to finance, however this just seems to be a coincidence since only one bucket is created, and it appears that the articles is not grouped very well when all of them ends up in the same bucket.\\

\textbf{Output - permutations: 5}\\
Based on the three articles, in the bucket containing four, they have topics: (interest,gnp) ,(cpi), (reserves) as they are related to money, they can seem similar, however they have different categories and thus it is hard to conclude their similarity.\\
~\\
The 3 articles in the big bucket has topics: (earn), (acq), and (earn), and can be seen as somewhat similar. \\
~\\
\textbf{Output - permutations: 10}\\
The three articles from the big bucket (69 articles), all has the topic "earn", So it seems valid that these are in the same bucket. \\
~\\
The second bucket (with 13 articles) also seem to be related to finance, two has topic "earn" while one has "money-supply". \\
It seems strange however that these are not then grouped in the same bucket.
~\\

It seems the higher number of permutations results in larger spread of buckets.
In hindsight we could have compared the topics of all articles in each bucket and checked for similarity this way, however we chose to look at both the article body as well as the topic for a sample of the articles grouped together in buckets, and the results on their similarity seem higher for higher permutations but are in the end inconclusive.

\end{document}
