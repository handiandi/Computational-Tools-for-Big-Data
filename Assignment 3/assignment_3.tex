\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage{amsmath,amssymb,graphicx}
\usepackage{cleveref}
\usepackage{url}
\usepackage{graphicx}
\usepackage[export]{adjustbox}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal
\DeclareUnicodeCharacter{00D6}{Ö}
\DeclareUnicodeCharacter{00DC}{Ü}
\DeclareUnicodeCharacter{00E4}{ä}
\DeclareUnicodeCharacter{00E4}{ä}

% Custom colors
\usepackage{color}

\definecolor{output_background}{rgb}{0.2, 0.2, 0.2}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}
\definecolor{purple}{rgb}{0.6, 0.1, 0.9}
\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=python,
breaklines=true,
basicstyle=\ttfamily\small,
otherkeywords={1, 2, 3, 4, 5, 6, 7, 8 ,9 , 0, -, =, +, [, ], (, \), \{, \}, :, *, !},             % Add keywords here
keywordstyle=\color{blue},
emph={class, pass, in, for, while, if, is, elif, else, not, and, or, OR
    def, print, exec, break, continue, return},
emphstyle=\color{black}\bfseries,
emph={[2]True, False, None, self},
emphstyle=[2]\color{purple},
emph={[3]from, import, as},
emphstyle=[3]\color{blue},
stringstyle=\color{red},
frame=tb,
showstringspaces=false,
morecomment=[s]{"""}{"""},
commentstyle=\color{gray},
rulesepcolor=\color{blue},
title=\lstname
}}

% Python style for output highlighting
\newcommand\pythonoutputstyle{\lstset{
backgroundcolor=\color{output_background},
rulecolor=\color{output_background},
basicstyle=\ttm\small\color{white},
showstringspaces=false,
inputencoding=utf8,
extendedchars=true,
literate=%
    {á}{{\'a}}1
    {č}{{\v{c}}}1
    {ď}{{\v{d}}}1
    {é}{{\'e}}1
    {ě}{{\v{e}}}1
    {í}{{\'i}}1
    {ň}{{\v{n}}}1
    {ó}{{\'o}}1
    {ř}{{\v{r}}}1
    {š}{{\v{s}}}1
    {ť}{{\v{t}}}1
    {ú}{{\'u}}1
    {ů}{{\r{u}}}1
    {ý}{{\'y}}1
    {ž}{{\v{z}}}1
    {Á}{{\'A}}1
    {Č}{{\v{C}}}1
    {Ď}{{\v{D}}}1
    {É}{{\'E}}1
    {Ě}{{\v{E}}}1
    {Í}{{\'I}}1
    {Ň}{{\v{N}}}1
    {Ó}{{\'O}}1
    {Ř}{{\v{R}}}1
    {Š}{{\v{S}}}1
    {Ť}{{\v{T}}}1
    {Ú}{{\'U}}1
    {Ů}{{\r{U}}}1
    {Ý}{{\'Y}}1
    {Ž}{{\v{Z}}}1
    {Ö}{{\"{O}}}1
    {ö}{{\"{o}}}1
    {Ü}{{\"{U}}}1
    {ü}{{\"{u}}}1   
    {ß}{{\ss}}1
    {ä}{{\"{a}}}1
    {é}{{\'{e}}}1
    {ô}{{\^{o}}}1
    {â}{{\^{a}}}1
    {è}{{\`{e}}}1
}}


\lstnewenvironment{pythonOutput}[1][]
{
\pythonoutputstyle
\lstset{#1}
}
{}

% Python for inline
\newcommand\pythonoutput[1]{{\pythonoutputstyle\lstinline!#1!}}


% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

\title{Assignment 3\\02807 Computational Tools for Big Data}
\author{S \& A}
\date{26th October 2015}
\usepackage[cm]{fullpage}
\begin{document}

\maketitle
\newpage
%-------------- Week 5 ---------------------
\section{Exercise 8.1}
\textbf{Short recap of the exercise}\\
\textit{Define and implement a MapReduce job to count the occurrences of each word in a text file. Document that it works with a small example.}\\
~\\
\textbf{Overview}\\
We implement the MapReduce jobs by creating mappers and reducers. A mapper takes key/value pairs and generates key/value pairs, a reducer iterates through values for each distinct key, and generates an output.\\\\
For this exercise, the mapper takes a line as value, then splits them and yields each word and the count 1. The reducer sums the value for each unique key and yields the result. 
\\*
~\\
\textbf{Code}
\pythonexternal{"../Lesson 8/exercise-8-1.py"}
~\\
\textbf{Input}
\begin{pythonOutput}
der var engang en mand
der boede i en spand
spanden var af ler
nu kan jeg ikke mer
\end{pythonOutput}
\textbf{Output}
\begin{pythonOutput}
"af"    1
"boede"    1
"der"   2
"en"    2
"engang"  1
"i" 1
"ikke"  1
"jeg"   1
"kan"   1
"ler"   1
"mand"  1
"mer"   1
"nu"    1
"spand" 1
"spanden"   1
"var"   2
\end{pythonOutput}
\textbf{Explanation of output and discussion}\\
We can see it returns the key (word) and the sum of the ones, thus the word count.~\\

\section{Exercise 8.2}
\textbf{Short recap of the exercise}\\
\textit{Define and implement a MapReduce job that determines if a graph has an Euler tour (all vertices have even degree) where you can assume that the graph you get is connected.
It is fine if you split the file into five different files. You do not need to keep the node and edge counts in the top of the file.}\\
~\\
\textbf{Overview}\\
For this exercise we use one mapper and two reducers.~\\
The steps method tells MrJob in which order the methods should run. In our case the mapper and reducer should run, and then reducer2. reducer2 reduces the yield result from reducer, where the logic is as follows:

\begin{itemize}
  \item \textbf{mapper} takes each line and split it into from- and to-edges. Then it yields these two as key with value 1
  \item \textbf{reducer} sums the values of the key, as we did in exercise 8.1
  \item \textbf{reducer2} checks if all values of the key is even\\
\end{itemize}
~\\
We  split the input file in five files and get a concatenation of outputs with the following bash command: ~\\
ls graph*.txt | xargs -n1 ./exercise-2.py -q \\
~\\
This runs the script with all graph files as arguments in ascending order.

~\\
\textbf{Code}
\pythonexternal{"../Lesson 8/exercise-2.py"}
~\\
\textbf{Output}
\begin{pythonOutput}
"Has Euler Tour"    true
"Has Euler Tour"    false
"Has Euler Tour"    true
"Has Euler Tour"    true
"Has Euler Tour"    false

\end{pythonOutput}


\section{Exercise 8.3}
\textbf{Short recap of the exercise}\\
\textit{Implement the MapReduce job from the lecture which finds common friends (note that for the Facebook file, you need to extend the job to convert from a list of edges to the format from the slides – do this with an additional map/reduce job).}\\
~\\
\textbf{Overview}\\
We split this assignment in two: one for the example in the slides, and one for the facebook file. ~\\
\textbf{For the slides example}

\textbf{For the facebook file}
~\\
\textbf{Code - Slides example}
\pythonexternal{"../Lesson 8/exercise-3-1.py"}
\textbf{Output - Slides example}
\begin{pythonOutput}
["A", "B"]  ["C", "D"]
["A", "C"]  ["B", "D"]
["A", "D"]  ["B", "C"]
["B", "C"]  ["A", "D", "E"]
["B", "D"]  ["A", "C", "E"]
["B", "E"]  ["C", "D"]
["C", "D"]  ["A", "B", "E"]
["C", "E"]  ["B", "D"]
["D", "E"]  ["B", "C"]


\end{pythonOutput}
\textbf{Code - Facebook file}
\pythonexternal{"../Lesson 8/exercise-3-2.py"}
\textbf{Output - Facebook file}
\begin{pythonOutput}
[0, 100]    [119, 150, 163, 189, 217, 269, 323, 64]
[0, 101]    [180, 187, 194, 204, 24, 242, 249, 254, 266, 299, 302, 317, 330, 346, 53, 80, 92, 
             94]
[0, 102]    [175, 227, 263, 296, 99]
[0, 103]    [136, 169, 172, 185, 200, 211, 25, 252, 271, 285, 323, 339, 56, 7, 98]
[0, 104]    [109, 113, 122, 123, 128, 142, 169, 186, 188, 200, 203, 21, 212, 239, 25, 252, 26, 
             271, 277, 295, 303, 318, 322, 325, 332, 344, 45, 55, 56, 67, 98]
[0, 105]    [119, 148, 21, 236, 25, 257, 272, 277, 280, 315, 39, 69, 9]
[0, 106]    [169, 231, 238, 29, 329, 332, 88]
[0, 107]    [171, 58]
[0, 108]    [127, 159, 184, 197, 21, 251, 272, 281, 284, 320, 36, 57]
[0, 109]    [104, 118, 119, 122, 13, 142, 148, 158, 169, 186, 200, 203, 21, 229, 239, 252, 26, 
             271, 277, 285, 295, 297, 303, 304, 31, 314, 322, 323, 324, 325, 331, 332, 50, 56, 
             67, 98]


\end{pythonOutput}


\section{Exercise 8.4}
\textbf{Short recap of the exercise}\\
\textit{Make a MapReduce job which counts the number of triangles in a graph.}\\
~\\
\textbf{Overview}\\
\textit{convert\_mapper}, \textit{convert\_reducer} and \textit{main\_mapper} is the same as in exercise 8.3. ~\\
We will describe the other mappers and reducers. The description can also be seen in the code.\\
 ~\\
\textit{main\_reducer} finds common "friends" (nodes) of its key-values. The new key is the original key and the new friend ( [old\_key, friend] ). It yields the new key and the value 1. \\
~\\
We then use \textit{main\_reducer2} to reduce by key and sums the value. \\
~\\
\textit{main\_mapper2} checks if both length of key is 3 and value is 3. If it is, it yield 1 as value, else 0. The key is the summation of what we wil find: "Number of Triangles" \\
~\\
We then use \textit{main\_reducer3} to reduce by key and sums the value, which gives us the final result. \\

~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 8/exercise-8-4.py"}
~\\
\textbf{Output}
\begin{pythonOutput}
./exercise-8-4-v2.py 8.4_small_test.txt -q
"Number of Triangles"   120676
\end{pythonOutput}
\textbf{Explanation of output and discussion}\\
We can see we found 120676 triangles in the graph of the road network of California.~\\
From their website\footnote{\url{http://www.cise.ufl.edu/research/sparse/matrices/SNAP/roadNet-CA.html}} we can see their number of triangles found corresponds with ours. 


%-------------- Week 9 ---------------------
\section{Exercise 9.1}
\textbf{Short recap of the exercise}\\
\textit{Write a Spark job to count the occurrences of each word in a text file. Document that it works with a small example}\\
~\\
\textbf{Overview}
~\\
Following is a summarization of the comments in the file. 
\begin{enumerate}
  \item Extract lines in the file
  \item Splitting the lines into words
  \item Creating a pair for each word in the form ( word , count ) where count is the occurrence of the word , set to 1
  \item Counting the words
  \item Get the result and sort it based on count
  \item Print out the result\\
\end{enumerate}
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 9/exercise-9-1.py"}~\\
\textbf{Output}~\\
\begin{pythonOutput}
'en' has an occurrences of 3
'der' has an occurrences of 2
'var' has an occurrences of 2
'spand' has an occurrences of 1
'spanden' has an occurrences of 1
'af' has an occurrences of 1
'i' has an occurrences of 1
'kan' has an occurrences of 1
'booede' has an occurrences of 1
'ikke' has an occurrences of 1
'ler' has an occurrences of 1
'mand' has an occurrences of 1
'gang' has an occurrences of 1
'mer' has an occurrences of 1
'nu' has an occurrences of 1
'jeg' has an occurrences of 1
\end{pythonOutput}
~\\
\textbf{Explanation of output and discussion}\\
We can see that the output is identical with exercise 8.1, and since we have validated the result i 8.1, this result is also correct. 

\section{Exercise 9.2}
\textbf{Short recap of the exercise}\\
\textit{Write a Spark job that determines if a graph has an Euler tour (all vertices have even degree) where you can assume that the graph you get is connected.~\\
It is fine if you split the file into 5 different files. You do not need to keep the node and edge counts in the top of the file.}\\
~\\
\textbf{Overview}
As in the exercise in Week 8, we have splittet the file into 5 different files. ~\\
We have created a python function 'has\_euler\_tour' which take a filename string as argument. \\
~\\
The following is a step-by-step explaination of the code (primarily the function). 
\begin{enumerate}
  \item Loading in the file
  \item Creating a pair of integer values in the form (from\_node , to\_node), by splitting on whitespace
  \item Creating a list of pairs in the form [(from\_node , to\_node), (to\_node , from\_node)]
  \item Creating a pair for each node in position 1 of the pairs in the list. The new pair has the form (node, count) where count is the occurrence of the node, set to 1
  \item We use reduceByKey to summerize the values. reduceByKey returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function a+b. 
  \item filter-function find those whos values are not even
  \item If the count of result from filter-function is 0, return true, else return false 
  \item Print out the result\\
\end{enumerate}
What the code actually does, is to count the edges for each node. When we create the list of opposite pairs (step 3), we account for both to and from edges, which mean we can simply count the occurences of the node in positon 1 (step 4). ~\\
From this we can decide whether it has Euler tour. 
~\\
\textbf{Code}\\
\pythonexternal{"../Lesson 9/exercise-9-2.py"}~\\
\textbf{Output}~\\
\begin{pythonOutput}
Has Euler Tour: True
Has Euler Tour: False
Has Euler Tour: True
Has Euler Tour: True
Has Euler Tour: False
\end{pythonOutput}
~\\
\textbf{Explanation of output and discussion}\\
We can see that the output is identical with exercise 8.2, so we assume this result is correct. 

 ~\\

\section{Exercise 9.3}
\textbf{Short recap of the exercise}\\

\textit{Compute the following things using Spark:
\begin{enumerate}
    \item What are the 10 networks I observed the most, and how many times were they observed? Note: the bssid is unique for every network, the name (ssid) of the network is not necessarily unique.
    \item What are the 10 most common wifi names? (ssid)
    \item What are the 10 longest wifi names? (again, ssid)
\end{enumerate}}
\textbf{Overview}~\\
We have created a single file with the code for all three tasks, for each task we will explain the relevant functions used if they differ from the previous task. ~\\

\textbf{What are the 10 networks I observed the most, and how many times were they observed?}~\\
We load the wifi.data file, then we extract bssid from it by converting it to a dict usin the eval() function. We make a pair (bssid, count) where count is 1. Then we add the counts together with reduceByKey and print out top 10 in descending order (by count). ~\\

 \textbf{What are the 10 most common wifi names? (SSID)}~\\
We extract distinctive pairs of SSIDs and BSSID using the distinct function, but omit duplicate access points.
We then count the SSIDs and print them like in the previous exercise. ~\\

 \textbf{What are the 10 longest wifi names? (SSID)}~\\
We get the distinct SSID, Then we print out top 10 in descending order by using the negative length of the SSID as the key. ~\\~\\ 
\textbf{Code}
\pythonexternal{"../Lesson 9/exercise-9-3.py"}~\\
\textbf{Output - What are the 10 networks I observed the most, and how many times were they observed? Note: the BSSID is unique for every network, the name (SSID) of the network is not necessarily unique.}
\begin{pythonOutput}
[(u'34:21:09:12:6c:1a', 347),
 (u'00:24:b2:98:39:d2', 338),
 (u'34:21:09:12:6c:18', 324),
 (u'e8:08:8b:c9:c1:79', 318),
 (u'44:94:fc:56:08:fb', 315),
 (u'00:22:b0:b3:f2:ea', 314),
 (u'2c:b0:5d:ef:08:2b', 272),
 (u'44:94:fc:56:ce:5e', 240),
 (u'28:cf:e9:84:a1:c3', 211),
 (u'bc:ee:7b:55:1a:43', 210)]
\end{pythonOutput}
\textbf{Output - What are the 10 most common wifi names? (SSID)}
\begin{pythonOutput}
[(u'', 15),
 (u'SIT-GUEST', 15),
 (u'SIT-BYOD', 13),
 (u'SIT-PROD', 13),
 (u'PDA-105', 11),
 (u'KNS-105', 11),
 (u'MED-105', 11),
 (u'KEA-PUBLIC', 6),
 (u'GST-105', 5),
 (u'wireless', 4)]
\end{pythonOutput}
\textbf{Output - What are the 10 longest wifi names? (SSID)}
\begin{pythonOutput}
[u'HP-Print-43-Deskjet 3520 series',
 u'TeliaGatewayA4-B1-E9-2C-9E-CA',
 u'TeliaGateway08-76-FF-84-FF-8C',
 u'TeliaGateway9C-97-26-57-15-F9',
 u'TeliaGateway08-76-FF-46-3E-36',
 u'TeliaGateway9C-97-26-57-15-99',
 u'TeliaGateway08-76-FF-8A-EE-32',
 u'TeliaGateway08-76-FF-85-04-2F',
 u'TeliaGateway08-76-FF-9C-E0-82',
 u'Charlotte R.s Wi-Fi-netv\xe6rk']

\end{pythonOutput}
\textbf{Explanation of code and discussion}
From the outputs we can see that the network with BSSID 34:21:09:12:6c:1a was observed the most, the most common wifi name was empty string and SIT-GUEST and the longest wifi name was HP-Print-43-Deskjet 3520 series.


%-------------- Week 10 ---------------------
\section{Exercise 10.1}
\textbf{Short recap of the exercise}\\
\textit{Explain in your own words what Deep Learning is.}~\\
\textbf{Answer}\\
Deep Learning (DL) is a subset/branch of Machine Learning (which is the study of algorithms which can improve themselves over time).\\

DL is used as any other Machine Learning technique, to learn from data, and predict an outcome based on the trained model. DL is based on Artificial Neural Networks (ANN - a Machine Learning model), an ANN is a machine learning model inspired by the neural network of the brain, it consists of a large number of interconnected weighted neurons which can activate each other and the weights can be tuned which makes it adaptable and able to learn. The difference between ANN and DL is, that DL trains multi-layered or hierarchical ANN's.\\

A DL model is able to use a technique called representational learning to learn features of the dataset, thus when you give it a raw representation of the data (that is human interpretable), it learns itself what the different features are and it's then able to do recognition/supervised learning.\\

The ANN's each learns a feature through a nonlinear transformation and pass it along to the next ANN in the hierarchy until it gets to a classifier. Through each ANN the features learned are higher representations of the data. For example for facial recognition, for the first ANN receiving the input data, the feature learned is edges, then the next ANN learns different combinations of edges forming a part of a face, and so on.\\

Deep learning is very effective dealing with unstructured data like media (sounds, images, videos, text, time-series data)\\

It can be used for recommender systems and thus try to predict a rating or peference an user would give to an item, this is used currently for media streaming services like Spotify or Netflix, which can recommend new music or shows based on your playlist.\\

\section{Exercise 10.2}
\textbf{Short recap of the exercise}\\
\textit{Explain in your own words what a Convolutional Neural Network is.}\\
~\\
\textbf{Answer}\\
A Convolutional Neural Network (CNN) is a variant of the Artificial Neural Network (ANN). It is inspired by the human visual system and is typically used in image recognition.\\

It works similarly to a (ANN) by using layers of neurons where each layer feeds into the next layer and for each layer, the layer will obtain higher-level features.\\

However where in an ANN each neuron in a given layer feeds into all the neurons in the next layer, in a CNN a subset or "patch" of neurons feeds into a number of neurons in the next layer, that patch of neurons is called the local receptive field, for image recognition, this means small "overlapping windows" of the input image is building the next layer resulting in a slightly smaller layer than the former. In a CNN each layer will have the same weights and bias and thus detect the same feature just at different windows in the image. Using the same weights and bias reduce the parameters involved and computational power needed.\footnote{\url{http://neuralnetworksanddeeplearning.com/chap6.html}}\\

A CNN often makes use of a max-pooling layer, in between convolutional layers, which takes a maximum of the features over segments of the input layer, to reduce the spatial size of the data thus reducing the amount of parameters, and makes the model invariant to very small transformations of the data (overfitting).\footnote{\url{http://colah.github.io/posts/2014-07-Conv-Nets-Modular/}}.\\

With neural network (and other commonly machine learning techniques), feature engineering is one of the most essential and important task for good classification\footnote{\url{http://blog.kaggle.com/2014/08/01/learning-from-the-best/}}. Feature engineering is a very hard and complex task, which you would have to do for each type of problems. Using CNN, feature engineering is beeing done when we train it. 
For each training, it gets better and better to filter the input for relevant information (feature engineering). Once we learned our hierarchical features, we can simply pass them to a fully connected, simple neural network that combines them in order to classify the input into classes\footnote{\url{http://timdettmers.com/2015/03/26/convolution-deep-learning/}}.\\

\section{Exercise 10.3}
\textbf{Short recap of the exercise}\\
\textit{In this exercise you will be using the Caffe Deep Learning framework to classify a picture of a cat, and a picture of your own choosing.}\\
~\\
\textbf{Overview}\\
We started the virtual machine by running \textit{vagrant up} and navigating via our browser to \textit{localhost:8003}\\
We executed the code contained in the Jupyter notebook with a series of pictures: the default cat picture, a picture of a pig as seen in Figure \ref{fig:pig}, a picture of a beer glass \ref{fig:beer_glass}, a picture of a cloud and below each picture we got an output.
~\\
\textbf{Output - cat}\\
\begin{pythonOutput}
Predicted class is #282.
['n02123159 tiger cat' 'n02123045 tabby, tabby cat'
 'n02124075 Egyptian cat' 'n02119022 red fox, Vulpes vulpes'
 'n02127052 lynx, catamount']
\end{pythonOutput}
\textbf{Output - pig}
\begin{figure}[h!]
\begin{center}
\caption{Picture of a pig.}
\label{fig:pig}
\includegraphics[scale=0.2]{"../Lesson 10/images/pig"}
\end{center}
\end{figure}
\begin{pythonOutput}
Predicted class is #341.
['n02395406 hog, pig, grunter, squealer, Sus scrofa'
 'n01632777 axolotl, mud puppy, Ambystoma mexicanum'
 'n03223299 doormat, welcome mat' 'n02328150 Angora, Angora rabbit'
 'n02412080 ram, tup']
\end{pythonOutput}
\textbf{Output - beer glass}
\begin{figure}[h!]
\begin{center}
\caption{Picture of a beer glass.}
\label{fig:beer_glass}
\includegraphics[scale=0.5]{"../Lesson 10/images/beer_glass"}
\end{center}
\end{figure}
\begin{pythonOutput}
Predicted class is #855.
['n04423845 thimble' 'n02823750 beer glass'
 'n07615774 ice lolly, lolly, lollipop, popsicle'
 'n04131690 saltshaker, salt shaker' 'n02948072 candle, taper, wax light']
\end{pythonOutput}

\textbf{Output - cloud}
\begin{figure}[h!]
\begin{center}
\caption{Picture of cloud.}
\label{fig:cloud}
\includegraphics[scale=0.5]{"../Lesson 10/images/cloud"}
\end{center}
\end{figure}
\begin{pythonOutput}
Predicted class is #190.
['n02095889 Sealyham terrier, Sealyham' 'n04266014 space shuttle'
 'n09288635 geyser' 'n02948072 candle, taper, wax light' 'n03825788 nipple']
\end{pythonOutput}
\textbf{Explanation of code and discussion}\\

The predicted class for the default cat picture results in a correct classification as "tabby" or "tabby cat" among the top 4 answers, we then tried the pig picture in Figure \ref{fig:pig} which we can read resulted in the correct classification as hog/pig. Next we tried somewhat to trick it by showing it an alternatively shaped beer glass as shown in Figure \ref{fig:beer_glass}, this was also correctly identified in the second answer, next we tried a cloud as seen in Figure \ref{fig:cloud}, and it seemed to do the trick as none of the answers given by the model were correct.
%-------------- Week 11 ---------------------
\section{Exercise 11.1}
\textbf{Short recap of the exercise}\\
\textit{Train a random forest classifier to predict the topic earn in the articles, then implement it using feature hashing with 1000 buckets}\\
~\\
\textbf{Overview}
\textbf{Code}\\
\pythonexternal{"../Lesson 11/exercise_11_1.py"}
~\\
\textbf{Output}
\begin{pythonOutput}
Accuracy using a bag-of-words representation: 
(10377, 70793)
0.958092485549
Accuracy using feature-hashing with 2 buckets:
(10377, 2)
0.721579961464
Accuracy using feature-hashing with 10 buckets:
(10377, 10)
0.874759152216
Accuracy using feature-hashing with 100 buckets:
(10377, 100)
0.926782273603
Accuracy using feature-hashing with 500 buckets:
(10377, 500)
0.933526011561
Accuracy using feature-hashing with 1000 buckets:
(10377, 1000)
0.939306358382


\end{pythonOutput}
\textbf{Explanation of code and discussion}\\
We implemented feature-hashing using the Exercise description and the Lesson 11 slides\footnote{\url{https://www.dropbox.com/s/8vh8nyywttf5y6e/hashing.pdf?dl=0}}.\\ In the articles dataset we found that 3776 articles contained the topic earn out of the total of 10377 articles, by simply guessing this would mean an accuracy of 63.611\%. In the output we can see the shape and accuracies of feature-hashing with a number of buckets. We find that even with only 2 buckets, the accuracy is 72.157\%, and for a bigger number of buckets the accuracy is nearing the accuracy of the bag-of-words representation. For the bag-of-words representation with 70793 features the accuracy is 95.809\% and for feature-hashing with 1000 buckets it is 93.930\%.

\section{Exercise 11.2}
\textbf{Short recap of the exercise}\\
\textit{}\\
~\\
\textbf{Overview}\\

~\\
\textbf{Code}\\
%\pythonexternal{"../Lesson 7/flajolet_martin.py"}
~\\
\textbf{Output}
\begin{pythonOutput}

\end{pythonOutput}
\textbf{Explanation of code and discussion}\\


\end{document}
